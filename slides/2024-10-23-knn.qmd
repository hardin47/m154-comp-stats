---
title: "$k$-Nearest Neighbors"
author: "Jo Hardin"
subtitle: "October 23, 2024"
format: 
  revealjs:
    incremental: false
    scrollable: true
    slide-number: true
    show-slide-number: all
    embed-resources: true
    html-math-method: mathjax
execute:
  echo: true
  warning: false
  message: false
bibliography: 
  - ../slides.bib
---


```{r include=FALSE}
library(tidyverse)
library(mosaic)
library(boot)
library(skimr)
```



# Agenda 10/23/24

1. Redux - model process
2. cross validation
3. $k$-Nearest Neighbors



## `tidymodels` syntax

1. partition the data
2. build a recipe
3. select a model
4. create a workflow
5. fit the model  
6. validate the model



## All together

```{r echo = FALSE}
library(tidymodels)
library(palmerpenguins)

set.seed(47)
penguin_split <- initial_split(penguins)
penguin_train <- training(penguin_split)
penguin_test <- testing(penguin_split)
```

::: {.panel-tabset}

## recipe
```{r}
#| message: true
penguin_lm_recipe <-
  recipe(body_mass_g ~ species + island + bill_length_mm + 
           bill_depth_mm + flipper_length_mm + sex + year,
         data = penguin_train) |>
  step_mutate(year = as.factor(year)) |>
  step_unknown(sex, new_level = "unknown") |>
  step_relevel(sex, ref_level = "female") |>
  update_role(island, new_role = "id variable")

penguin_lm_recipe
```

## model
```{r}
penguin_lm <- linear_reg() |>
  set_engine("lm")

penguin_lm
```

## workflow
```{r}
penguin_lm_wflow <- workflow() |>
  add_model(penguin_lm) |>
  add_recipe(penguin_lm_recipe)

penguin_lm_wflow
```

## fit
```{r}
penguin_lm_fit <- penguin_lm_wflow |>
  fit(data = penguin_train)

penguin_lm_fit |> tidy()
```

:::



## model parameters

* Some model parameters are tuned from the data (some aren't).
  - linear model coefficients are optimized (not tuned)
  - $k$-nn value of $k$ is tuned

* If the model is tuned using the data, the same data **cannot** be used to assess the model.

* With Cross Validation, you iteratively put data in your pocket.

* For example, keep 1/5 of the data in your pocket, build the model on the remaining 4/5 of the data.


## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide2.png")
```


## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide3.png")
```


## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide4.png")
```



## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide5.png")
```


## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide6.png")
```


## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide7.png")
```


## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide8.png")
```


## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide9.png")
```


## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide10.png")
```


## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide11.png")
```





## model parameters

* Some model parameters are tuned from the data (some aren't).
  - linear model coefficients are optimized (not tuned)
  - $k$-NN value of $k$ is tuned

* If the model is tuned using the data, the same data **cannot** be used to assess the model.

* With Cross Validation, you iteratively put data in your pocket.

* For example, keep 1/5 of the data in your pocket, build the model on the remaining 4/5 of the data.



## $k$-Nearest Neighbors

The $k$-Nearest Neighbor algorithm does exactly what it sounds like it does.  

* user decides on the integer value for $k$

* user decides on a distance metric (most $k$-NN algorithms default to Euclidean distance)

* a point is classified to be in the same group as the majority of the $k$ **closest** points in the training data.



##  $k$-NN visually

Consider a population, a training set, and a decision boundary:

```{r fig.cap = "image credit: Ricardo Gutierrez-Osuna", fig.alt = "Population structure is shown as 2D concentric rings.  Dataset is given by a sample of points from the rings.  Decision boundary is a jagged space roughy approximating the concentric rings.", out.width='80%', fig.align='center', echo=FALSE}
knitr::include_graphics("../images/knnmodel.jpg")
```




## $k$-NN visually

Choosing $k$ accurately is one of the most important aspects of the algorithm.

```{r fig.cap = "image credit: Ricardo Gutierrez-Osuna", fig.alt = "For each of k = 1, 5, 20, the resulting decision boundary is shown.  Certainly with k=20 the decision boundary does not approximate the population.", out.width='80%', fig.align='center', echo=FALSE}
knitr::include_graphics("../images/knnK.jpg")
```



## $k$-NN to predict penguin species


::: {.panel-tabset}

## recipe
```{r}
#| message: true

penguin_knn_recipe <-
  recipe(species ~ body_mass_g + island + bill_length_mm + 
           bill_depth_mm + flipper_length_mm,
         data = penguin_train) |>
  update_role(island, new_role = "id variable") |>
  step_normalize(all_predictors())

penguin_knn_recipe
```

## model
```{r}
penguin_knn <- nearest_neighbor() |>
  set_engine("kknn") |>
  set_mode("classification")

penguin_knn
```

## workflow
```{r}
penguin_knn_wflow <- workflow() |>
  add_model(penguin_knn) |>
  add_recipe(penguin_knn_recipe)

penguin_knn_wflow
```

## fit
```{r}
penguin_knn_fit <- penguin_knn_wflow |>
  fit(data = penguin_train)
```

## predict
```{r}
penguin_knn_fit |> 
  predict(new_data = penguin_test) |>
  cbind(penguin_test) |>
  metrics(truth = species, estimate = .pred_class) |>
  filter(.metric == "accuracy")
```

:::




##  what is $k$ ???

It turns out that the default value for $k$ in the **kknn** engine is 7.  Is 7 best?


#### Cross Validation!!!

The red observations are used to fit the model, the black observations are used to assess the model.

```{r echo = FALSE, fig.align = 'center', fig.cap = "Image credit: Alison Hill", out.width="60%"}
knitr::include_graphics("../images/CV/Slide11.png")
```



## Cross validation

Randomly split the training data into V distinct blocks of roughly equal size.

* leave out the first block of analysis data and fit a model.
* the model is used to predict the held-out block of assessment data.
* continue the process until all V assessment blocks have been predicted.

The tuned parameter is usually chosen to be the one which produces the best performance averaged across the V blocks.

The final performance is usually based on the test data.



##  Extending the modeling process


::: {.panel-tabset}

## creating folds
```{r}
set.seed(470)
penguin_vfold <- vfold_cv(penguin_train,
                          v = 3, strata = species)
```

## k
```{r}
k_grid <- data.frame(neighbors = seq(1, 15, by = 4))
k_grid
```

## tune wkflow
```{r}
penguin_knn_tune <- nearest_neighbor(neighbors = tune()) |>
  set_engine("kknn") |>
  set_mode("classification")

penguin_knn_wflow_tune <- workflow() |>
  add_model(penguin_knn_tune) |>
  add_recipe(penguin_knn_recipe)
```

## tuning
```{r}
penguin_knn_wflow_tune |>
  tune_grid(resamples = penguin_vfold, 
           grid = k_grid) |>
  collect_metrics() |>
  filter(.metric == "accuracy")
```

:::



##  We choose $k$ = 9 !

### 6. Validate the model


::: {.panel-tabset}

## recipe
```{r}
#| message: true

penguin_knn_recipe <-
  recipe(species ~ body_mass_g + island + bill_length_mm + 
           bill_depth_mm + flipper_length_mm,
         data = penguin_train) |>
  update_role(island, new_role = "id variable") |>
  step_normalize(all_predictors())

penguin_knn_recipe
```

## model
```{r}
penguin_knn_final <- nearest_neighbor(neighbors = 9) |>
  set_engine("kknn") |>
  set_mode("classification")

penguin_knn_final
```

## workflow
```{r}
penguin_knn_wflow_final <- workflow() |>
  add_model(penguin_knn_final) |>
  add_recipe(penguin_knn_recipe)

penguin_knn_wflow_final
```

## fit
```{r}
penguin_knn_fit_final <- penguin_knn_wflow_final |>
  fit(data = penguin_train)
```

## predict
```{r}
penguin_knn_fit_final |> 
  predict(new_data = penguin_test) |>
  cbind(penguin_test) |>
  metrics(truth = species, estimate = .pred_class) |>
  filter(.metric == "accuracy")
```

:::



##  We choose $k$ = 9 !

### 6. Validate the model

Huh.  Seems like $k=9$ didn't do as well as $k=7$ (the value we tried at the very beginning before cross validating).

Well, it turns out, that's the nature of variability, randomness, and model building.

We don't know truth, and we won't every find a perfect model.



## Bias-Variance Tradeoff

```{r fig.cap = "Test and training error as a function of model complexity.  Note that the error goes down monotonically only for the training data.  Be careful not to overfit!!  image credit: ISLR", out.width='90%', fig.align='center', echo=FALSE}
knitr::include_graphics("../images/varbias.png")
```



## Reflecting on Model Building

```{r echo = FALSE, fig.cap = "Image credit: https://www.tmwr.org/"}
knitr::include_graphics("../images/modelbuild1.png")
```



## Reflecting on Model Building

```{r echo = FALSE, fig.cap = "Image credit: https://www.tmwr.org/"}
knitr::include_graphics("../images/modelbuild2.png")
```





## Reflecting on Model Building

```{r echo = FALSE, fig.cap = "Image credit: https://www.tmwr.org/", out.width = "70%", fig.align='center'}
knitr::include_graphics("../images/modelbuild3.png")
```


