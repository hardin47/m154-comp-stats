---
title: "Simulating"
author: "Jo Hardin"
subtitle: "September 8 - 17, 2025"
format: 
  revealjs:
    incremental: false
    scrollable: true
    slide-number: true
    show-slide-number: all
    embed-resources: true
    math: mathjax
execute:
  echo: true
  warning: false
  message: false
bibliography: 
  - ../slides.bib
--- 


```{r include=FALSE}
library(tidyverse)
library(knitr)
library(lubridate)
library(openintro)
```


# Agenda  9/23/24

1. Simulating to understand bias-variance trade-off
2. `map()`

# Bias-Variance Trade-off

## What is variance? What is bias?

**Variance** refers to the amount by which $\hat{f}$
 would change if we estimated it using a different training set. 

**Bias** refers to the error that is introduced by approximating the "truth" by a model which is too simple. 


## Mathematically

We can measure how well a model does by looking at how close the fit of the model ($\hat{f}$) is to the observed data ($y$). 

$$MSE = E\bigg[(y - \hat{f}(x))^2\bigg]$$

Because $\hat{f}(x) = f(x) + \epsilon$, we know that:

\begin{eqnarray*}
MSE &=& E\bigg[(y - \hat{f}(x))^2\bigg]\\
&=& E\bigg[(f(x) + \epsilon - \hat{f}(x))^2\bigg]\\
&=& E\bigg[(f(x) - \hat{f}(x))^2\bigg] + 2 E\bigg[(f(x) - \hat{f}(x)) \cdot \epsilon \bigg] + E[\epsilon^2]
\end{eqnarray*}

Term 3: The third term is $\sigma^2$, the irreducible error (the part that even the perfect model can't predict).

Term 2: Turns out to be zero because the errors are assumped to be independent of the explanatory variable, so the expected value can filter through.

Term 1: Takes a little bit of work to expand (something you'd do in, say, Math 152)^[The expected value here is over the model $\hat{f}(x)$, not over the random variable $X$. That is, $\epsilon$ is the thing that varies.].

## Simplified Term 1

$$E\bigg[(f(x) - \hat{f}(x))^2\bigg] = \bigg( f(x) - E[\hat{f}(x)] \bigg)^2 + E\bigg[ (E[\hat{f}(x)] - \hat{f}(x))^2 \bigg]$$

Where:

$\bigg[$ Bias of $\hat{f}(x) \bigg]^2$ is $\bigg( f(x) - E[\hat{f}(x)] \bigg)^2$

Variance of $\hat{f}(x)$ is $E\bigg[ (E[\hat{f}(x)] - \hat{f}(x))^2 \bigg]$ 

## MSE

\begin{eqnarray*}
MSE &=& E\bigg[(y - \hat{f}(x))^2\bigg]\\
&=& \bigg[ \mbox{ Bias of } \hat{f}(x) \bigg]^2 + \mbox{ Variance of } \hat{f}(x) + \sigma^2
\end{eqnarray*}

## Simulation

Let's consider a model: $Y = f(x) + \epsilon$

::: {.panel-tabset}

## plot
```{r}
#| echo: false

num_x <- length(seq(0, 10, by = 0.5))
set.seed(4774)
bias_var_data <- tibble(ex = seq(0, 10, by = 0.5),
                        eps = rnorm(num_x, mean = 0, sd = 5),
                        why = ex^2 + eps) |> 
  mutate(underfit = lm(why ~ ex)$fitted,
         `good fit` = lm(why ~ ex + I(ex^2))$fitted,
         overfit = lm(why ~ ex + I(ex^2) + I(ex^3) + I(ex^4) + I(ex^5) + I(ex^6) + I(ex^7) + I(ex^8) + I(ex^9) + I(ex^10) )$fitted) |> 
  pivot_longer(cols = underfit:overfit, 
               names_to = "fit", 
               values_to = "prediction") |> 
  mutate(fit = factor(fit, 
                        levels = c("underfit", "good fit", "overfit") ))

bias_var_data |> 
  ggplot(aes(x = ex, y = why)) + 
  geom_point() + 
  geom_line(aes(y = prediction)) + 
  facet_grid(~ fit)


```

## code
```{r}
#| eval: false

num_x <- length(seq(0, 10, by = 0.5))
set.seed(4774)
bias_var_data <- tibble(ex = seq(0, 10, by = 0.5),
                        eps = rnorm(num_x, mean = 0, sd = 5),
                        why = ex^2 + eps) |> 
  mutate(underfit = lm(why ~ ex)$fitted,
         `good fit` = lm(why ~ ex + I(ex^2))$fitted,
         overfit = lm(why ~ ex + I(ex^2) + I(ex^3) + I(ex^4) + I(ex^5) + I(ex^6) + I(ex^7) + I(ex^8) + I(ex^9) + I(ex^10) )$fitted) |> 
  pivot_longer(cols = underfit:overfit, 
               names_to = "fit", 
               values_to = "prediction") |> 
  mutate(fit = factor(fit, 
                        levels = c("underfit", "good fit", "overfit") ))

bias_var_data |> 
  ggplot(aes(x = ex, y = why)) + 
  geom_point() + 
  geom_line(aes(y = prediction)) + 
  facet_grid(~ fit)


```
:::

## Errors

Which is worse? Underfitting or overfitting? How can we measure the **error**? 

::: {.panel-tabset}

## result
```{r}
#| echo: false
bias_var_data |> 
  group_by(fit) |> 
  summarize(MSE_OG = mean((why - prediction)^2))
```

## code
```{r}
#| eval: false
bias_var_data |> 
  group_by(fit) |> 
  summarize(MSE_OG = mean((why - prediction)^2))
```

:::

## Fit to new data

WAIT! We don't want the error on the data that built the model, we want the error on the population.


::: {.panel-tabset}
## results
```{r}
#| echo: false
set.seed(470)
new_data <- tibble(ex = seq(0, 10, by = 0.5),
                   eps = rnorm(num_x, mean = 0, sd = 5),
                   why = ex^2 + eps)

bias_var_data |> 
  ggplot(aes(x = ex, y = why)) + 
  geom_point(data = new_data, aes(x = ex, y = why)) + 
  geom_line(aes(y = prediction)) + 
  facet_grid(~ fit)
```

```{r}
#| echo: false

lm1 <- bias_var_data |> 
  filter(fit == "underfit") |> 
  lm(why ~ ex, data = _)

lm2 <- bias_var_data |> 
  filter(fit == "good fit") |> 
  lm(why ~ ex + I(ex^2), data = _)

lm3 <- bias_var_data |> 
  filter(fit == "overfit") |> 
  lm(why ~ ex + I(ex^2) + I(ex^3) + I(ex^4) + I(ex^5) + 
       I(ex^6) + I(ex^7) + I(ex^8) + I(ex^9) + I(ex^10), data = _)

new_data |> 
  mutate(underfit = predict.lm(lm1, newdata = new_data),
         `good fit` = predict(lm2, newdata = new_data),
         overfit = predict(lm3, newdata = new_data)) |> 
  pivot_longer(cols = underfit:overfit, 
               names_to = "fit", 
               values_to = "prediction") |> 
  mutate(fit = factor(fit, 
                        levels = c("underfit", "good fit", "overfit") )) |> 
  group_by(fit) |> 
  summarize(MSE_newdata = mean((why - prediction)^2))
  
```

## code
```{r}
#| eval: false
set.seed(470)
new_data <- tibble(ex = seq(0, 10, by = 0.5),
                   eps = rnorm(num_x, mean = 0, sd = 5),
                   why = ex^2 + eps)

bias_var_data |> 
  ggplot(aes(x = ex, y = why)) + 
  geom_point(data = new_data, aes(x = ex, y = why)) + 
  geom_line(aes(y = prediction)) + 
  facet_grid(~ fit)
```

```{r}
#| eval: false

lm1 <- bias_var_data |> 
  filter(fit == "underfit") |> 
  lm(why ~ ex, data = _)

lm2 <- bias_var_data |> 
  filter(fit == "good fit") |> 
  lm(why ~ ex + I(ex^2), data = _)

lm3 <- bias_var_data |> 
  filter(fit == "overfit") |> 
  lm(why ~ ex + I(ex^2) + I(ex^3) + I(ex^4) + I(ex^5) + 
       I(ex^6) + I(ex^7) + I(ex^8) + I(ex^9) + I(ex^10), data = _)

new_data |> 
  mutate(underfit = predict.lm(lm1, newdata = new_data),
         `good fit` = predict(lm2, newdata = new_data),
         overfit = predict(lm3, newdata = new_data)) |> 
  pivot_longer(cols = underfit:overfit, 
               names_to = "fit", 
               values_to = "prediction") |> 
  mutate(fit = factor(fit, 
                        levels = c("underfit", "good fit", "overfit") )) |> 
  group_by(fit) |> 
  summarize(MSE_newdata = mean((why - prediction)^2))
  
```
:::


## Back to bias and variance

::: {.panel-tabset}
## model 1: `y ~ x`
```{r}
#| echo: false
set.seed(47)
datasets <- 1:9 |> 
  map_dfr(\(i) {
    tibble(
      ex  = seq(0, 10, by = 0.5),
      eps = rnorm(num_x, mean = 0, sd = 5),
      why = ex^2 + eps,
      dataset = i
    )
  }) |> 
  group_by(dataset) |> 
  mutate(underfit = lm(why ~ ex)$fitted,
         `good fit` = lm(why ~ ex + I(ex^2))$fitted,
         overfit = lm(why ~ ex + I(ex^2) + I(ex^3) + I(ex^4) + I(ex^5) + I(ex^6) + I(ex^7) + I(ex^8) + I(ex^9) + I(ex^10) )$fitted) |> 
  pivot_longer(cols = underfit:overfit, 
               names_to = "fit", 
               values_to = "prediction") |> 
  mutate(fit = factor(fit, 
                        levels = c("underfit", "good fit", "overfit") ))

```


```{r}
#| echo: false
datasets |> 
  filter(fit == "underfit") |> 
  ggplot(aes(x = ex, y = why)) + 
  geom_point(size = 0.25) + 
  geom_line(aes(y = prediction)) + 
  facet_wrap(~ dataset)
```

## model 2: `y ~ x + x^2`
```{r}
#| echo: false
datasets |> 
  filter(fit == "good fit") |> 
  ggplot(aes(x = ex, y = why)) + 
  geom_point(size = 0.25) + 
  geom_line(aes(y = prediction)) + 
  facet_wrap(~ dataset)
```

## model 3: `y ~ x + x^2 + x^3 + x^4 + x^5 + x^6 + x^7 + x^8 + x^9 + x^10`

```{r}
#| echo: false
datasets |> 
  filter(fit == "overfit") |> 
  ggplot(aes(x = ex, y = why)) + 
  geom_point(size = 0.25) + 
  geom_line(aes(y = prediction)) + 
  facet_wrap(~ dataset)
```

## code
```{r}
#| eval: false
set.seed(47)
datasets <- 1:9 |> 
  map_dfr(\(i) {
    tibble(
      ex  = seq(0, 10, by = 0.5),
      eps = rnorm(num_x, mean = 0, sd = 5),
      why = ex^2 + eps,
      dataset = i
    )
  }) |> 
  group_by(dataset) |> 
  mutate(underfit = lm(why ~ ex)$fitted,
         `good fit` = lm(why ~ ex + I(ex^2))$fitted,
         overfit = lm(why ~ ex + I(ex^2) + I(ex^3) + I(ex^4) + I(ex^5) + I(ex^6) + I(ex^7) + I(ex^8) + I(ex^9) + I(ex^10) )$fitted) |> 
  pivot_longer(cols = underfit:overfit, 
               names_to = "fit", 
               values_to = "prediction") |> 
  mutate(fit = factor(fit, 
                        levels = c("underfit", "good fit", "overfit") ))

datasets
```


```{r}
#| eval: false
datasets |> 
  filter(fit == "underfit") |> 
  ggplot(aes(x = ex, y = why)) + 
  geom_point(size = 0.25) + 
  geom_line(aes(y = prediction)) + 
  facet_wrap(~ dataset)
```



```{r}
#| eval: false
datasets |> 
  filter(fit == "good fit") |> 
  ggplot(aes(x = ex, y = why)) + 
  geom_point(size = 0.25) + 
  geom_line(aes(y = prediction)) + 
  facet_wrap(~ dataset)
```

```{r}
#| eval: false
datasets |> 
  filter(fit == "overfit") |> 
  ggplot(aes(x = ex, y = why)) + 
  geom_point(size = 0.25) + 
  geom_line(aes(y = prediction)) + 
  facet_wrap(~ dataset)
```

:::

## What we know about bias-variance trade-off

* The <span style = "color:purple"><u>underfit model</u></span> looks the same for every dataset! That is, the model has **low variance**.
* The <span style = "color:purple"><u>underfit model</u></span> doesn't fit the data very well. The model has **high bias**.


* The <span style = "color:teal"><u>overfit model</u></span> model is very flexible, and looks like it fits the data very well.  It has **low bias**.
* The fitted models look quite different across the different datasets. The <span style = "color:teal"><u>overfit model</u></span> has **high variance**.



## Goals of Simulating Complicated Models

The goal of simulating a complicated model is not only to create a program which will provide the desired results.  We also hope to be able to write code such that:

1. The problem is broken down into small pieces.
2. The problem has checks in it to see what works (run the lines *inside* the functions!).
3. uses simple code (as often as possible).


## Aside: a few R functions (`ifelse()`)
```{r}
data.frame(value = c(-2:2)) |>
  mutate(abs_value = ifelse(value >=0, value, -value))  # abs val
```


## Aside: a few R functions (`case_when()`)
```{r}
set.seed(4747)
diamonds |> select(carat, cut, color, price) |>
  sample_n(20) |>
  mutate(price_cat = case_when(
    price > 10000 ~ "expensive",
    price > 1500 ~ "medium", 
    TRUE ~ "inexpensive"))
```

## Aside: a few R functions (`sample()`)

*sampling*, *shuffling*,  and *resampling*: `sample()` 

```{r}
set.seed(47)
alph <- letters[1:10]
alph

sample(alph, 5, replace = FALSE) # sample (from a population)

sample(alph, 5, replace = TRUE) # sample (from a population)

sample(alph, 10, replace = FALSE)  # shuffle

sample(alph, 10, replace = TRUE)  # resample
```

## Aside: a few R functions (`set.seed()`)

What if we want to be able to generate the **same** random numbers (here on the interval from 0 to 1) over and over?

```{r}
set.seed(4747)
runif(4, 0, 1) # random uniform numbers

set.seed(123)
runif(4, 0, 1) # random uniform numbers

set.seed(4747)
runif(4, 0, 1) # random uniform numbers
```


## Why?

Three simulation methods are used for different purposes:

1. Monte Carlo methods - use repeated *sampling* from a **population** with known characteristics.

2. Randomization / Permutation methods - use *shuffling* (sampling without replacement from a **sample**) to test hypotheses of "no effect".


3. Bootstrap methods - use *resampling* (sampling with replacement from a **sample**) to establish confidence intervals.



## Monte Carlo from a population to...


```{=html}
<style>
.semi-transparent {
  opacity: 0.5;
}
</style>
```


* ...simulate as easier than calculus for estimating probabilities.
* [...simulate to understand complicated models.]{.semi-transparent}
* [...simulate to consider statistical methods with fewer assumptions.]{.semi-transparent}

# Small simulation example

## Sally & Joan

Consider a situation where Sally and Joan plan to meet to study in their college campus center [@mosteller1987;@MDSR]. They are both impatient people who will wait only 10 minutes for the other before leaving.

But their planning was incomplete. Sally said, "Meet me between 7 and 8 tonight at the student center." When should Joan plan to arrive at the campus center? And what is the probability that they actually meet?

## Simulate their meeting

Assume that Sally and Joan are both equally likely to arrive at the campus center anywhere between 7pm and 8pm. 

::: {.panel-tabset}

## vectorized 

```{r}
library(tictoc)
n <- 10000

tic()
meet_vect <- tibble(
  sally = runif(n, 0, max = 60),
  joan = runif(n, min = 0, max = 60),
  result = ifelse(
    abs(sally - joan) <= 10, "They meet", "They do not"
  )
)
toc()
```
## function

```{r}

meet_func <- function(nada){
  tibble(
    sally = runif(1, min = 0, max = 60),
    joan = runif(1, min = 0, max = 60),
    result = ifelse(
    abs(sally - joan) <= 10, "They meet", "They do not"
  )
  )
}
```


## `map()`

```{r}
tic()
meet_map <- 1:n |> 
  map(meet_func) |> 
  list_rbind()
toc()
```

## `for()` loop

```{r}
tic()
meet_for <- data.frame()
for(i in 1:n){
  meet_for <- meet_for |> rbind(
    tibble(
      sally = runif(1, min = 0, max = 60),
      joan = runif(1, min = 0, max = 60),
      result = ifelse(
    abs(sally - joan) <= 10, "They meet", "They do not"
  )))
}
toc()

```

:::


##  Results

The results themselves are equivalent.  Differing values due to randomness in the simulation.

::: {.panel-tabset}

## vectorized

```{r}
meet_vect |> 
  group_by(result) |> 
  summarize(number = n()) |> 
  mutate(proprotion = number / sum(number))
```

## `map()`

```{r}

meet_map |> 
  group_by(result) |> 
  summarize(number = n())|> 
  mutate(proprotion = number / sum(number))
```

## `for()` loop

```{r}
meet_for |> 
  group_by(result) |> 
  summarize(number = n())|> 
  mutate(proprotion = number / sum(number))
```

:::

## Visualizing the meet up


```{r}
meet_map |> 
  ggplot(aes(x = joan, y = sally, color = result)) +
  geom_point(alpha = 0.3) + 
  geom_abline(intercept = 10, slope = 1) + 
  geom_abline(intercept = -10, slope = 1) + 
  scale_color_brewer(palette = "Paired")
```


## Simulation best practices

* no magic numbers
* comment your code
* use informative names
* set a seed for reproducibility


# Bigger simulation example

## Thoughts on simulating

* break down the problem into **small** steps
* check to see what works
* simple is better

<!--
## Roulette

* A roulette wheel has 38 slots numbered 00, 0, and 1â€“36.
Two are green, 18 are red, and 18 are black.
* If a gambler bets based on color, the return on a $1 bet
is $2
* A gambler has $20, and will continuously bet $1 on red
until they double their money (have $40) or lose the
money they came with
* What is the probability the gambler doubles their money?

Question: Without calculating probabilities, how could you
design an experiment to estimate this probability?

The example is taken directly from [Stat 279, Fall 2023](https://sta279-f23.github.io/) with Ciaran Evans. 

## Design the simulation

Step 1: Need a Roulette wheel!  ... and money  
Step 2: Will spin the wheel  
Step 3: Depending on the wheel, update the money.  
  * if spin is red: `money + 1`  
  * if spin is not red: `money - 1`  
Step 4: Keep spinning until `money = 40` or `money = 0`  
Step 5: Repeat many times  

## Step 1: create a roulette wheel

```{r}
wheel <- c(rep("green", 2), rep("black", 18), rep("red", 18))

wheel
```


* `rep()` repeats a value the specified number of times
* `c()` combines vectors (or scalars) into a single vector

## Step 2: spin the wheel

```{r}
spin <- sample(wheel, size = 1)

spin
```

## Step 3: update the money

```{r}
money <- 20

spin <- sample(wheel, size = 1)
money <- ifelse(spin == "red", money + 1, money  - 1)

spin
money
```




## Step 4: keep spinning (function first)

```{r}
spin_func <- function(money = 20, i){
  wheel <- c(rep("green", 2), rep("black", 18), rep("red", 18))
  spin <- sample(wheel, size = 1)
  money <- ifelse(spin == "red", money + 1, money  - 1)
  money
}

spin_func(10)
spin_func(spin_func(10))
```

## Aside: the `accumulate()` function

In R, the `accumulate()` function will run `f(f(f(f(x))))` as many times as asked.

```{r}
1:10 |> 
  accumulate(`+`)

1:10 |> 
  accumulate(`+`, .init = 47)
```

## Step 4: run multiple times

```{r}
set.seed(4747)
1:20 |> accumulate(spin_func, .init = 20)
```

## Step 4: another function

```{r}
game_func <- function(rep, money, iter){
  data.frame(interim_money = 1:iter |> accumulate(spin_func, .init = money),
             game = 0:iter,
             rep = rep)
}

game_func(1, 20, 3)
```


## Step 5: Repeat many times

Note that `map()` is specifically designed to work in vectorized ways, so it doesn't have an easy `while()` companion.  Which means that we need to make sure to `iter`ate long enough to either lose or double our money.

```{r}
output <- 1:20 |> map(game_func, money = 20, iter = 100000) |> 
  list_rbind()

output
```



## Step 6: Wrangle the output

```{r}
end <- output |> 
  group_by(rep) |> 
  filter(interim_money >= 40 | interim_money <= 0) |> 
  group_by(rep) |> 
  slice_min(game) |> 
  ungroup()

end
```

## Step 6: Wrangle the output

```{r}
end |> 
  summarize(prop_double = mean(interim_money == 40))
```


## Without magic numbers

```{r}
set.seed(4747)
money <- 10
games <- 20
iterations <- 10000


1:games |> map(game_func, money = money, iter = iterations) |> 
  list_rbind() |> 
  group_by(rep) |> 
  filter(interim_money >= 2 * money | interim_money <= 0) |> 
  group_by(rep) |> 
  slice_min(game) |> 
  ungroup() |> 
  summarize(prop_double = mean(interim_money == 2 * money))
```
-->


# Agenda  9/25/24

1. Simulating to understand complicated models
2. Simulating to consider statistical methods with fewer assumptions

# Modeling

* [Simulation as easier than calculus for estimating probabilities.]{.semi-transparent}
* Simulation to understand complicated models.
* [Simulation to consider statistical methods with fewer assumptions.]{.semi-transparent}


<!--
## Bias in a model

Population:

```
talent ~ Normal (100, 15)
grades ~ Normal (talent, 15)
SAT ~ Normal (talent, 15)
```


The example is taken directly (and mostly verbatim) from a blog by Aaron Roth [Algorithmic Unfairness Without Any Bias Baked In](http://aaronsadventures.blogspot.com/2019/01/discussion-of-unfairness-in-machine.html).  


## Goal for admitting students

College wants to admit students with 

> `talent > 115` 

... but they only have access to `grades` and `SAT` which are noisy estimates of `talent`.


## Plan for accepting students

* Run a regression on a training dataset (`talent` is known for existing students)
* Find a model which predicts `talent` based on `grades` and `SAT`
* Choose students for whom predicted `talent` is above 115


##  Flaw in the plan ...

* there are two populations of students, the Reds and Blues. 
* Reds are the majority population (99%), and Blues are a small minority population (1%) 
* the Reds and the Blues are no different when it comes to talent: they both have the same talent distribution, as described above. 
* there is no bias baked into the grading or the exams: both the Reds and the Blues also have exactly the same grade and exam score distributions

## What is really different?

> But there is one difference: the Blues have more money than the Reds, so they each take the **SAT twice**, and report only the highest of the two scores to the college. This results in a small but noticeable bump in their average SAT scores, compared to the Reds.



##  Key aspect:

> The value of `SAT` means something different for the Reds versus the Blues

(They have different feature distributions.)


##  Let's see what happens ...

```{r echo=FALSE}
set.seed(470)
n_obs <- 100000
n.red <- n_obs*0.99
n.blue <- n_obs*0.01
reds <- rnorm(n.red, mean = 100, sd = 15)
blues <- rnorm(n.blue, mean = 100, sd = 15)

red.sat <- reds + rnorm(n.red, mean = 0, sd = 15)
blue.sat <- blues + 
    pmax(rnorm(n.blue, mean = 0, sd = 15),
         rnorm(n.blue, mean = 0, sd = 15))

red.grade <- reds + rnorm(n.red, mean = 0, sd = 15)
blue.grade <- blues + rnorm(n.blue, mean = 0, sd = 15)

college.data <- data.frame(talent = c(reds, blues),
                           SAT = c(red.sat, blue.sat),
                           grades = c(red.grade, blue.grade),
                           color = c(rep("red", n.red), rep("blue", n.blue)))

ggplot(college.data, aes(x = grades, y = SAT, color = color)) +
  geom_point(size = 0.5) +
  scale_color_identity(name = "Color Group",
                       guide = "legend") +
  geom_abline(intercept = 0, slope = 1)
  
```


## Two models:

```{r echo = FALSE}
red.lm = college.data |>
  dplyr::filter(color == "red") %>%
  lm(talent ~ SAT + grades, data = .)

blue.lm = college.data |>
  dplyr::filter(color == "blue") %>%
  lm(talent ~ SAT + grades, data = .)
```

Red model (SAT taken once):
```{r echo = FALSE}
red.lm |> broom::tidy()
```
Blue model (SAT is max score of two):
```{r echo = FALSE}
blue.lm |> broom::tidy()
```



## New data

* Generate new data, use the **two** models separately. 

* Can the models predict if a student has `talent` > 115?

```{r echo=FALSE}
set.seed(4747)
new.reds <- rnorm(n.red, mean = 100, sd = 15)
new.blues <- rnorm(n.blue, mean = 100, sd = 15)

new.red.sat <- new.reds + rnorm(n.red, mean = 0, sd = 15)
new.blue.sat <- new.blues + 
    pmax(rnorm(n.blue, mean = 0, sd = 15),
         rnorm(n.blue, mean = 0, sd = 15))

new.red.grade <- new.reds + rnorm(n.red, mean = 0, sd = 15)
new.blue.grade <- new.blues + rnorm(n.blue, mean = 0, sd = 15)

new.college.data <- data.frame(talent = c(new.reds, new.blues),
                           SAT = c(new.red.sat, new.blue.sat),
                           grades = c(new.red.grade, new.blue.grade),
                           color = c(rep("red", n.red), rep("blue", n.blue)))


new.red.pred <- new.college.data |>
  filter(color == "red") %>%
  predict.lm(red.lm, newdata = .)

new.blue.pred <- new.college.data |>
  filter(color == "blue") %>%
  predict.lm(blue.lm, newdata = .)

new.college.data <- new.college.data |> 
  cbind(predicted = c(new.red.pred, new.blue.pred))

ggplot(new.college.data, aes(x = talent, y = predicted, color = color)) + 
  geom_point(size = 0.5) + 
  geom_hline(yintercept = 115) + 
  geom_vline(xintercept = 115) +
  scale_color_identity(name = "Color Group",
                       guide = "legend")
```



## New data

:::: {.columns}

::: {.column width=50%}
```{r echo=FALSE}
ggplot(new.college.data, aes(x = talent, y = predicted, color = color)) + 
  geom_point(size = 0.5) + 
  geom_hline(yintercept = 115) + 
  geom_vline(xintercept = 115) +
  scale_color_identity(name = "Color Group",
                       guide = "legend")
```
:::


::: {.column width=50%}
```{r echo=FALSE}
new.college.data <- new.college.data |> 
  mutate(fp = ifelse(talent < 115 & predicted > 115, 1, 0),
         tp = ifelse(talent > 115 & predicted > 115, 1, 0),
         fn = ifelse(talent > 115 & predicted < 115, 1, 0),
         tn = ifelse(talent < 115 & predicted < 115, 1, 0))

error.rates <- new.college.data |> group_by(color) |>
  summarize(tpr = sum(tp) / (sum(tp) + sum(fn)),
            fpr = sum(fp) / (sum(fp) + sum(tn)),
            fnr = sum(fn) / (sum(fn) + sum(tp)),
            #fdr = sum(fp) / (sum(fp) + sum(tp)),
            error = (sum(fp) + sum(fn)) / (sum(fp) + sum(tp) + sum(fn) + sum(tn) ))

error.rates
```
:::

::::



## **TWO** models doesn't seem right????

What if we fit only one model to the entire dataset?

After all, there are laws against using protected classes to make decisions (housing, jobs, money loans, college, etc.)

```{r echo = FALSE}
global.lm = college.data %>%
  lm(talent ~ SAT + grades, data = .)

global.lm |> broom::tidy()
```

(The coefficients kinda look like the red model...)


## How do the error rates change?

:::: {.columns}

::: {.column width=50%}
```{r echo=FALSE}
new.pred <- new.college.data %>%
  predict.lm(global.lm, newdata = .)

new.college.data <- new.college.data |> 
  cbind(global.predicted = new.pred)

ggplot(new.college.data, aes(x = talent, 
                             y = global.predicted, 
                             color = color)) + 
  geom_point(size = 0.5) + 
  geom_hline(yintercept = 115) + 
  geom_vline(xintercept = 115) +
  scale_color_identity(name = "Color Group",
                       guide = "legend")
```
:::

::: {.column width=50%}
One model:
```{r echo = FALSE}
new.college.data.glb <- new.college.data |> 
  mutate(fp = ifelse(talent < 115 & global.predicted > 115, 1, 0),
         tp = ifelse(talent > 115 & global.predicted > 115, 1, 0),
         fn = ifelse(talent > 115 & global.predicted < 115, 1, 0),
         tn = ifelse(talent < 115 & global.predicted < 115, 1, 0))

error.rates <- new.college.data.glb |> group_by(color) |>
  summarize(tpr = sum(tp) / (sum(tp) + sum(fn)),
            fpr = sum(fp) / (sum(fp) + sum(tn)),
            fnr = sum(fn) / (sum(fn) + sum(tp)),
            #fdr = sum(fp) / (sum(fp) + sum(tp)),
            error = (sum(fp) + sum(fn)) / (sum(fp) + sum(tp) + sum(fn) + sum(tn) ))

error.rates
```

Two separate models:
```{r echo = FALSE}
new.college.data.2mod <- new.college.data |> 
  mutate(fp = ifelse(talent < 115 & predicted > 115, 1, 0),
         tp = ifelse(talent > 115 & predicted > 115, 1, 0),
         fn = ifelse(talent > 115 & predicted < 115, 1, 0),
         tn = ifelse(talent < 115 & predicted < 115, 1, 0))

error.rates <- new.college.data.2mod |> group_by(color) |>
  summarize(tpr = sum(tp) / (sum(tp) + sum(fn)),
            fpr = sum(fp) / (sum(fp) + sum(tn)),
            fnr = sum(fn) / (sum(fn) + sum(tp)),
            #fdr = sum(fp) / (sum(fp) + sum(tp)),
            error = (sum(fp) + sum(fn)) / (sum(fp) + sum(tp) + sum(fn) + sum(tn) ))

error.rates
```
:::
::::

##  What did we learn?

> with two populations that have different feature distributions, learning a single classifier (that is prohibited from discriminating based on population) will fit the bigger of the two populations

* depending on the nature of the distribution difference, it can be either to the benefit or the detriment of the minority population

* no explicit human bias, either on the part of the algorithm designer or the data gathering process

* the problem is exacerbated if we artificially force the algorithm to be group blind

* well-intentioned "fairness" regulations prohibiting decision makers form taking sensitive attributes into account can actually make things less fair and less accurate at the same time

## Simulate?

* different varying proportions
* effect due to variability
* effect due to SAT coefficient
* different number of times the blues get to take the test
* etc.


## Presidential election

```{r, out.width="40%", fig.alt = "As of September 24, 2024, The New York Times has Harris at 49% and Trump at 47% in the national polling average.", fig.cap = "Harris is ahead in the national polling but behind in North Carolina, Arizona, and Georgia.", echo=FALSE}
knitr::include_graphics("../images/nyt_Sep242024.png")
``` 


## Presidential election

```{r, out.width="40%", fig.alt = "As of September 24, 2024, 538 says that Harris wins in 57% of simulated elections, and Trump wins in 42% of simulated elections.", fig.cap = "538 simulates many many elections to see how the individual state percentages play out.", echo=FALSE}
knitr::include_graphics("../images/sim_538_Sep242024.png")
``` 

## Election Data

(Big caveat: the data are old!)

```{r}
#| echo: false
electoral_college <- read_csv("../data/electoral_college.csv") |> 
  separate_wider_delim(state, 
                       delim = " - ", 
                       names = c("state", "evotes")) |> 
  #mutate(evotes = as.numeric(str_extract(evotes, "[0-9]+")))
  mutate(evotes = parse_number(evotes)) |> 
  rbind(data.frame(state = c("ME-1", "ME-2", "NE-2"),
                   evotes = c(1,1,1))) |> 
  mutate(evotes = case_when(
    state == "Maine" ~ 2,
    state == "Nebraska" ~ 4,
    TRUE ~ evotes
  ))

```


```{r}
#| echo: false
pres_polls <- read_csv("../data/presidential_general_averages.csv") |> 
  filter(state != "National") |> 
  mutate(pct = ifelse(is.na(pct_trend_adjusted), pct_estimate,
                      pct_trend_adjusted)) |> 
  select(candidate, date, state, pct) |> 
  pivot_wider(names_from = candidate, values_from = pct) |> 
  mutate(Trump = ifelse(is.na(Trump), `Donald Trump`, Trump)) |> 
  mutate(Biden = ifelse(is.na(Biden), `Joseph R. Biden Jr.`, Biden)) |> 
  select(date, state, Harris, Trump, Biden) |> 
  mutate(Harris = ifelse(is.na(Harris), Biden, Harris)) |> 
  select(-Biden) |> 
  mutate(date = lubridate::parse_date_time(date, orders = "mdy")) |> 
  arrange(state, desc(date)) |> 
  group_by(state) |> 
  slice_head(n = 1)
  
```


```{r}
#| echo: false
poll_data <- pres_polls |> 
  full_join(electoral_college, by = "state") |> 
  ungroup()

poll_data
```

```{r}
#| echo: false
#| eval: false
write_csv(poll_data, "../data/poll_data.csv")
```


Data sources include: <a href = "https://github.com/fivethirtyeight/data/blob/master/polls/2024-averages/presidential_general_averages_2024-09-12_uncorrected.csv" target = "_blank">538</a> and the <a href = "https://www.archives.gov/electoral-college/allocation" target = "_blank">National Archives</a>. 

## Election model

* each state has a Bernoulli probability of going for either Harris or Trump
* the states are independent of one another
* the probabilities are an accurate model for how each state will vote

(Do we believe the model conditions?)

## Function to determine who wins

Need 270 to win!

```{r}
election <- function(i, data) {
  rand_number <- runif(nrow(data)) * 100
  data |> 
    cbind(rand_number) |> 
    mutate(state_winner = case_when(
      rand_number < Harris ~ "Harris",
      rand_number < Harris + Trump ~ "Trump",
      rand_number < (Harris + Trump) + (100 - Harris - Trump) /2 ~ "Harris",
      TRUE ~ "Trump")) |> 
    group_by(state_winner) |> 
    summarize(votes = sum(evotes)) |> 
    mutate(simulation = paste0("simulation", i))
}

election(47, poll_data)
```

## Running the function many times


```{r}
set.seed(47)
map(1:1000, election, data = poll_data) |> 
  list_rbind() |> 
  group_by(simulation) |> 
  slice_max(votes) |> 
  ungroup() |> 
  mutate(winner = case_when(
    votes >= 270 ~ state_winner,
    TRUE ~ "no winner"
  )) |> 
  select(winner) |> 
  table()
```

## Caveats

* The simulation assumes that each state is independent (538's model is much more sophisticated modeling connections across states and other demographic groups)
* The poll numbers are very old
* The poll numbers are constantly changing

-->

# Sensitivity of assumptions


* [Simulation as easier than calculus for estimating probabilities.]{.semi-transparent}
* [Simulation to understand complicated models..]{.semi-transparent}
* Simulation to consider statistical methods with fewer assumptions

## t-test with Poisson data

The t-test says that if the null hypothesis is true, we'd expect to reject a true null hypothesis $\alpha \cdot 100$ percent of the time.  That is, if $\alpha = 0.05$, we'd reject true null hypotheses 5% of the time.

::: .callout-note
The guarantee of 5% (across many tests) happens only when the data are normal (or reasonably normal).
:::

## Non-normal data

```{r}
data.frame(pois_data = rpois(n = 100000, lambda = 1)) |> 
  ggplot(aes(x = pois_data)) +
  geom_histogram()
```

## t-test function

```{r}
t.test.pval <- function(data1, data2){
  t.test(data1, data2) |> 
    broom::tidy() |> 
    select(p.value)
}

t.test.pval(rpois(n = 10, lambda = 1), rpois(n = 7, lambda = 1))

```

## t-test 10 times

```{r}
map(1:10, ~t.test.pval(rpois(n = 10, lambda = 1),
                       rpois(n = 7, lambda = 1))) |> 
          list_rbind()
```

## t-test many times

::: {.panel-tabset}

## `map()`

```{r}
set.seed(4747)
results <- map(1:10000, 
               ~t.test.pval(rpois(n = 8, lambda = 1),
                            rpois(n = 5, lambda = 1))) |> 
  list_rbind() 

```

## plot
```{r}
results |> 
  ggplot(aes(x = p.value)) + 
  geom_histogram(bins = 30) + 
  labs(x = "p.values")
```

## reject $H_0$

```{r}
results |> 
  summarize(type1 = mean(p.value < 0.05))
```
:::


# Sensitivity of CI to tech conditions

Consider the following set up:

```{r}
#| echo: false
library(ggplot2)
library(ggthemes)
library(broom)
```

```{r}
set.seed(474)
beta0 <- -1; beta1 <- 0.5; beta2 <- 1.5
n_obs <- 100; reps <- 1000


x1 <- rep(c(0,1), each=n_obs/2)
x2 <- runif(n_obs, min=-1, max=1)
y <- beta0 + beta1*x1 + beta2*x2 + rnorm(n_obs, mean=0, sd = 1)
```



## Plot of data
```{r echo = FALSE}
lm(y ~ x1 + x2) |>
  augment() |>
  ggplot(aes(x=x2, y = y, color=as.factor(x1))) + 
  geom_point() +
  geom_line(aes(y = .fitted)) +
  scale_colour_tableau("Color Blind") +
  guides(color=guide_legend(title="x1 value")) +
  xlab("x2 value") + ylab("y value")
```



## Capture the slope parameter?

* We know the truth (!!!!!)  
* We can create a single CI for the parameter given a single data set  
* Is the parameter in the interval?  
* Should it always be in the interval?  How often should it be?  
* Repeat the process to see whether the confidence interval captures the parameter at the claimed rate.  


See: [Rossman Chance regression applet](https://www.rossmanchance.com/applets/2021/regshuffle/regshuffle.htm)



## Running a linear model

```{r}
CI <- lm(y~x1+x2) |> tidy(conf.int=TRUE) |> data.frame()
CI

CI |>
  filter(term == "x2") |>
  select(term, estimate, conf.low, conf.high) |>
  mutate(inside = between(beta2, conf.low, conf.high))
```



## What happens to the CI of coefficients in repeated samples? (eq var)

::: {.panel-tabset}

## Datasets

```{r}
eqvar_data <- data.frame(row_id = seq(1, n_obs, 1)) |>
  slice(rep(row_id, each = reps)) |>
  mutate(
    sim_id = rep(1:reps, n_obs),
    x1 = rep(c(0,1), each = n()/2),
    x2 = runif(n(), min = -1, max = 1),
    y = beta0 + beta1*x1 + beta2*x2 + rnorm(n(), mean = 0, sd = 1)
  ) |>
  arrange(sim_id, row_id) |>
  group_by(sim_id) |>
  nest()

eqvar_data
``` 


## Function 
```{r}
beta_coef <- function(df){
  lm(y ~ x1 + x2, data = df) |>
    tidy(conf.int = TRUE) |>
    filter(term == "x2") |>
    select(estimate, conf.low, conf.high, p.value) 
}
```

## `map()`

```{r}
eqvar_data |> 
  mutate(b2_vals = map(data, beta_coef)) |>
  select(b2_vals) |> 
  unnest(b2_vals) |>
  summarize(capture = between(beta2, conf.low, conf.high)) |>
  summarize(capture_rate = sum(capture)/reps)
```

:::

## Sensitivity of CI to tech conditions

Consider the following set up (note the difference in variability):

```{r unequal-ci-1}
set.seed(470)
beta0 <- -1; beta1 <- 0.5; beta2 <- 1.5
n_obs <- 100; reps <- 1000

  x1 <- rep(c(0,1), each=n_obs/2)
  x2 <- runif(n_obs, min=-1, max=1)
  y <- beta0 + beta1*x1 + beta2*x2 + 
             rnorm(n_obs, mean=0, sd = 1 + x1 + 10*abs(x2))
```



## Plot of data
```{r echo = FALSE}
lm(y ~ x1 + x2) |>
  augment() |>
  arrange(x2) |>
  ggplot(aes(x=x2, y = y, color=as.factor(x1))) + 
  geom_point() +
  geom_line(aes(y = .fitted)) +
  scale_colour_tableau("Color Blind") +
  guides(color=guide_legend(title="x1 value")) +
  xlab("x2 value") + ylab("y value")
```


## What happens to the CI of coefficients in repeated samples? (uneq var)
```{r echo = FALSE}
set.seed(47)
```

::: {.panel-tabset}

## Datasets
```{r unequal-ci}
uneqvar_data <- data.frame(row_id = seq(1, n_obs, 1)) |>
  slice(rep(row_id, each = reps)) |>
  mutate(sim_id = rep(1:reps, n_obs), 
         x1 = rep(c(0,1), each = n()/2),
         x2 = runif(n(), min = -1, max = 1),
         y = beta0 + beta1*x1 + beta2*x2 + 
              rnorm(n(), mean = 0, sd = 1 + x1 + 10*abs(x2))  ) |>
  arrange(sim_id, row_id) |>
  group_by(sim_id) |> nest() 
```


## `map()`
```{r}
uneqvar_data |> 
  mutate(b2_vals = map(data, beta_coef)) |>
  select(b2_vals) |> unnest(b2_vals) |>
  summarize(capture = between(beta2, conf.low, conf.high)) |>
  summarize(capture_rate = sum(capture)/reps)
```
:::

## Equal variance: type I error?

Another method for thinking about type I error rates.  With equal variance, the type I error rate is close to what we'd expect, 5%.


::: {.panel-tabset}

## function
```{r}
t_test_pval <- function(df){
  t.test(y ~ x1, data = df, var.equal = TRUE) |>
    tidy() |>
    select(estimate, p.value) 
}
```


## generate data

```{r}
set.seed(470)
reps <- 1000
n_obs <- 20
null_data_equal <- 
  data.frame(row_id = seq(1, n_obs, 1)) |>
  slice(rep(row_id, each = reps)) |>
  mutate(
    sim_id = rep(1:reps, n_obs),
    x1 = rep(c("group1", "group2"), each = n()/2),
    y = rnorm(n(), mean = 10, 
               sd = rep(c(1,1), each = n()/2))
  ) |>
  arrange(sim_id, row_id) |>
  group_by(sim_id) |>
  nest()
```


## summarize p-values
```{r}
null_data_equal |> 
  mutate(t_vals = map(data,t_test_pval)) |>
  select(t_vals) |> 
  unnest(t_vals) |>
  ungroup(sim_id) |>
  summarize(type1error_rate = sum(p.value < 0.05)/reps)
```

:::



## Unequal variance: type I error?

With unequal variance, the type I error rate is much higher than we set.  We set the the type I error rate to be 5%, but the simulated rate was 6.36%.

::: {.panel-tabset}
```{r}
t_test_pval <- function(df){
  t.test(y ~ x1, data = df, var.equal = TRUE) |>
    tidy() |>
    select(estimate, p.value) 
}
```

## generate data
```{r unequal-ttest}
set.seed(470)
reps <- 1000
n_obs <- 20
null_data_unequal <- 
  data.frame(row_id = seq(1, n_obs, 1)) |>
  slice(rep(row_id, each = reps)) |>
  mutate(
    sim_id = rep(1:reps, n_obs),
    x1 = rep(c("group1", "group2"), each = n()/2),
    y = rnorm(n(), mean = 10, 
               sd = rep(c(1,100), each = n()/2))
  ) |>
  arrange(sim_id, row_id) |>
  group_by(sim_id) |>
  nest()
```



## summarize p-values
```{r}
null_data_unequal |> 
  mutate(t_vals = map(data,t_test_pval)) |>
  select(t_vals) |> 
  unnest(t_vals) |>
  ungroup(sim_id) |>
  summarize(type1error_rate = sum(p.value < 0.05)/reps)
```
:::



## References


