---
title: "Bagging and Random Forests"
author: "Jo Hardin"
subtitle: "November 4 + 6, 2024"
format:
  revealjs:
    incremental: false
    scrollable: true
    slide-number: true
    show-slide-number: all
    embed-resources: true
    html-math-method: mathjax
execute:
  echo: true
  warning: false
  message: false
bibliography: 
  - ../slides.bib
---


```{r include=FALSE}
library(tidyverse)
library(mosaic)
library(boot)
library(skimr)
```

# Agenda 11/04/2024

1. Redux - CART
2. bagging process
3. bagging error rate (OOB error)



## `tidymodels` syntax

1. partition the data
2. build a recipe
3. select a model
4. create a workflow
5. fit the model  
6. validate the model



## Random Forests

> Random forests are an ensemble learning method for classification (and regression) that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes output by individual trees -- [Wikipedia](http://en.wikipedia.org/wiki/Random_forest)



## Why a forest instead of a tree?

* Trees suffer from very **high variance**

* Two datasets will provide extremely different trees

* To reduce variability, fit many trees and aggregate the predictions.

* No longer reporting the tree structure, now reporting the predicted values (with hopes that predictions from one dataset will be similar to predictions from another dataset).


## Bagging

Bagging = **B**ootstrap **Agg**regating. 

* multiple models aggregate together will produce a smoother model fit and better balance between 
   - bias in the fit
   - variance in the fit.  
   
Bagging can be applied to any classifier to reduce variability.

<p style = "color:purple">Recall that the variance of the sample mean is variance of data / n.  So we've seen the idea that averaging an outcome gives reduced variability.</p>



## Bagging Models

```{r fig.cap = "Image credit: https://www.pluralsight.com/guides/ensemble-methods:-bagging-versus-boosting", out.width='70%', fig.align='center', echo=FALSE}
knitr::include_graphics("../images/bagging.png")
```



## Bagging Trees

```{r fig.cap = "Image credit: https://www.analyticsvidhya.com/blog/2021/05/bagging-25-questions-to-test-your-skills-on-random-forest-algorithm/", out.width='90%', fig.align='center', echo=FALSE}
knitr::include_graphics("../images/baggedtree.png")
```



## Algorithm:  Bagging Forest
1. Resample (bootstrap) *cases* (observational units, not variables).  
2. Build a tree on each new set of (bootstrapped) training observations.
3. Average (regression) or majority vote (classification).
4. Note that for every bootstrap sample, approximately 2/3 of the observations will be chosen and 1/3 of them will not be chosen.


## Observations not in a given tree


P(observation $i$ is not in the bootstrap sample) = $\bigg(1 - \frac{1}{n} \bigg)^n$


$$\lim_{n \rightarrow \infty} \bigg(1 - \frac{1}{n} \bigg)^n = \frac{1}{e} \approx \frac{1}{3}$$



## OOB Error

**Out of Bag**

with bagging, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally **for free**!!!

* Each tree is constructed using a different bootstrap sample 

* Put all of the cases left out in the construction of the $b^{th}$ tree down the $b^{th}$ tree to get a classification. 

* At the end of the run, 
   - take $j$ to be the class that got most of the votes every time case $i$ was oob. 
   - The proportion of times that $j$ is not equal to the true class averaged over all cases is the **oob error**. 

## OOB Prediction {.smaller}

Let the OOB prediction for the $i^{th}$ observation to be  $\hat{y}_{(-i)}$

\begin{eqnarray*}
\mbox{OOB}_{\mbox{error}} &=& \frac{1}{n} \sum_{i=1}^n \textrm{I} (y_i \ne \hat{y}_{(-i)}) \ \ \ \ \ \ \ \  \mbox{classification}\\
\mbox{OOB}_{\mbox{error}} &=& \frac{1}{n} \sum_{i=1}^n  (y_i - \hat{y}_{(-i)})^2  \ \ \ \ \ \ \ \ \mbox{regression}\\
\end{eqnarray*}

| obs | tree1 | tree2 | tree3 | tree4 | $\cdots$ | tree100 | average           |
|-----|:-----:|:-----:|:-----:|:-----:|:--------:|:-------:|--------------------|
|  1  |       |   X   |   X   |       |          |         | $\sum(pred)/38$    |
|  2  |   X   |       |       |       |          |         | $\sum(pred)/30$    |
|  3  |       |       |       |   X   |          |   X     | $\sum(pred)/33$    |
|  4  |   X   |       |       |       |          |         | $\sum(pred)/32$    |
|  5  |   X   |       |       |       |          |         | $\sum(pred)/39$    |
|  6  |       |       |   X   |       |          |   X     | $\sum(pred)/29$    |
|  7  |       |       |       |       |          |   X     | $\sum(pred)/29$    |
|  8  |       |       |   X   |   X   |          |   X     | $\sum(pred)/31$    |
|  9  |       |       |       |   X   |          |         | $\sum(pred)/36$    |




## Bagging example w defaults

```{r echo = FALSE}
library(tidymodels)
library(palmerpenguins)

set.seed(47)
penguin_split <- initial_split(penguins)
penguin_train <- training(penguin_split)
penguin_test <- testing(penguin_split)
```

::::: {.panel-tabset}

## recipe
```{r}
#| message: true
penguin_bag_recipe <-
  recipe(body_mass_g ~ . ,
         data = penguin_train) |>
  step_unknown(sex, new_level = "unknown") |>
  step_mutate(year = as.factor(year)) |>
  step_naomit(all_predictors(), body_mass_g)

penguin_bag_recipe
```

## model
```{r}
num_trees <- 500
penguin_bag <- rand_forest(mtry = 7,
                           trees = num_trees) |>
  set_engine("randomForest", oob.error = TRUE) |>
  set_mode("regression")

penguin_bag
```

## workflow
```{r}
penguin_bag_wflow <- workflow() |>
  add_model(penguin_bag) |>
  add_recipe(penguin_bag_recipe)

penguin_bag_wflow
```

## fit


:::: {.columns}
::: {.column width=50%}
```{r eval = FALSE}
penguin_bag_fit <- 
  penguin_bag_wflow |>
  fit(data = penguin_train)

penguin_bag_fit 
```
:::

::: {.column width=50%}
```{r echo = FALSE}
penguin_bag_fit <- 
  penguin_bag_wflow |>
  fit(data = penguin_train)

penguin_bag_fit 
```
:::

::::

:::::


## Fit again
```{r echo = FALSE}
#| message: true
penguin_bag_fit <- 
  penguin_bag_wflow |>
  fit(data = penguin_train)

penguin_bag_fit 
```



##  Plotting the OOB (not tidy)


::::: {.panel-tabset}

## plot 1

:::: {.columns}

::: {.column width=50%}
```{r eval = FALSE}
rsq <- penguin_bag_fit |>
  extract_fit_parsnip() |>
  pluck("fit") |>
  pluck("rsq")


data.frame(
  trees = 1:num_trees, 
  rsq = rsq) |>
  ggplot() + 
  geom_line(
    aes(x = trees, y = rsq)) +
  ylab("") + 
  xlab("Number of Trees") + 
  ggtitle("OOB R-squared")
```
:::

::: {.column width=50%}
```{r echo = FALSE}
rsq <- penguin_bag_fit |>
  extract_fit_parsnip() |>
  pluck("fit") |>
  pluck("rsq")  # or mse

data.frame(trees = 1:num_trees, rsq = rsq) |>
  ggplot() + 
  geom_line(aes(x = trees, y = rsq)) +
  ylab("") + xlab("Number of Trees") + 
  ggtitle("OOB R-squared")
```
:::

::::

## plot 2
:::: {.columns}

::: {.column width=50%}
```{r eval = FALSE}
MSE <- penguin_bag_fit |>
  extract_fit_parsnip() |>
  pluck("fit") |>
  pluck("mse")

data.frame(
  trees = 1:num_trees, 
  MSE = MSE) |>
  ggplot() + 
  geom_line(
    aes(x = trees, y = MSE)) +
  ylab("") + 
  xlab("Number of Trees") + 
  ggtitle("OOB MSE")
```
:::

::: {.column width=50%}
```{r echo = FALSE}
MSE <- penguin_bag_fit |>
  extract_fit_parsnip() |>
  pluck("fit") |>
  pluck("mse")  

data.frame(
  trees = 1:num_trees, 
  MSE = MSE) |>
  ggplot() + 
  geom_line(aes(x = trees, y = MSE)) +
  ylab("") + xlab("Number of Trees") + 
  ggtitle("OOB MSE")
```

:::

::::

:::::



# Agenda 11/06/24

1. Random Forests
2. Example


## Random Forests

* Many trees
  - bootstrap the data before building each tree
  - for best split look only at a random $m$ variables (different $m$ variables per split)
  
* Tuning:  look at OOB predictions -- either average or majority vote

* Testing:  test data will be (average or majority vote) prediction for all the trees.





## Algorithm:  Random Forest

1. Bootstrap sample from the training set.
2. Grow an un-pruned tree on the bootstrap sample.
* *At each split*, select $m$ variables and determine the best split using only the $m$ predictors.
* Do *not* prune the tree.  Save the tree as is!
3. Repeat steps 1-2 for many many trees.
4. For each tree grown on a bootstrap sample, predict the OOB samples.  For each tree grown, $~1/3$ of the training samples won't be in the bootstrap sample -- those are called out of bootstrap (OOB) samples.  OOB samples can be used as *test* data to estimate the error rate of the tree.
5. Combine the OOB predictions to create the "out-of-bag" error rate (either majority vote or average of predictions / class probabilities).
6. All trees together represent the *model* that is used for new predictions (either majority vote or average).




## Variable Importance

> importance = decrease in node impurity resulting from splits over that variable, averaged over all trees.


* For each tree, the prediction error on the out-of-bag portion of the data is recorded (error rate for classification, MSE for regression).  

* Keep the set of trees, and **permute** the $j^{th}$ variable and recalculate the oob prediction error.  

* The difference between the two oob errors are averaged over all trees (for the $j^{th}$ variable) to give the importance for the $j^{th}$ variable.




## RF technical details

* unfortunately, **tidymodels** currently does not have an OOB option for model tuning (e.g., to find `mtry`)

* unlike `num_trees`, `mtry` is not calculated "as you go along"

* cross validation will be used to find the optimal value of `mtry`



## RF example w CV tuning

```{r echo = FALSE}
library(tidymodels)
library(palmerpenguins)

penguins <- penguins |>
  drop_na()

set.seed(47)
penguin_split <- initial_split(penguins)
penguin_train <- training(penguin_split)
penguin_test <- testing(penguin_split)
```

::: {.panel-tabset}

## recipe
```{r}
#| message: true
penguin_rf_recipe <-
  recipe(body_mass_g ~ . ,
         data = penguin_train) |>
  step_unknown(sex, new_level = "unknown") |>
  step_mutate(year = as.factor(year)) 

penguin_rf_recipe
```

## model
```{r}
penguin_rf <- rand_forest(mtry = tune(),
                           trees = tune()) |>
  set_engine("ranger", importance = "permutation") |>
  set_mode("regression")

penguin_rf
```

## workflow

```{r}
penguin_rf_wflow <- workflow() |>
  add_model(penguin_rf) |>
  add_recipe(penguin_rf_recipe)

penguin_rf_wflow
```


## CV
```{r}
set.seed(234)
penguin_folds <- vfold_cv(penguin_train,
                          v = 4)
```

## param
```{r}
penguin_grid <- grid_regular(mtry(range = c(2,7)),
                             trees(range = c(1,500)), levels = 5)

penguin_grid
```

## tune
```{r}
penguin_rf_tune <- 
  penguin_rf_wflow |>
  tune_grid(resamples = penguin_folds,
            grid = penguin_grid)

penguin_rf_tune 
```
:::


##  RF model output

```{r fig.height = 5}
penguin_rf_tune |>
  collect_metrics() |>
  filter(.metric == "rmse") |>
  mutate(mtry = as.factor(mtry)) |> 
  ggplot() + 
  geom_line(aes(x = trees, y = mean, color = mtry)) + 
  labs(title = "RMSE: root mean squared error", y = "")
```





## RF Final model

```{r}
penguin_rf_best <- finalize_model(
  penguin_rf,
  select_best(penguin_rf_tune, metric = "rmse"))

penguin_rf_best


penguin_rf_final <-
  workflow() |>
  add_model(penguin_rf_best) |>
  add_recipe(penguin_rf_recipe) |>
  fit(data = penguin_train)
```



## RF Final model

```{r}
penguin_rf_final
```



# VIP 

```{r fig.height = 5}
library(vip)

penguin_rf_final |>
  extract_fit_parsnip() |>
  vip(geom = "point")
```



## Test predictions

```{r fig.height = 5}
penguin_rf_final |>
  predict(new_data = penguin_test) |>
  cbind(penguin_test) |>
  ggplot() +
  geom_point(aes(x = body_mass_g, y = .pred)) + 
  geom_abline(intercept = 0, slope = 1)
```



## Model choices

:::: {.columns}

::: {.column width=60%}

* explanatory variable choice                 
* number of explanatory variables             
* functions/transformation of explanatory                       
* transformation of response                
* response: continuous vs. categorical              
* how missing data is dealt with              
* train/test split (`set.seed`)                
* train/test proportion             
* type of classification model     
* use of cost complexity / parameter   
* majority / average prob (tree error rate)         
* accuracy vs sensitivity vs specificity         
:::

::: {.column width=40%}

* $k$ ($k$-NN) 
* distance measure
* V (CV)
* CV `set.seed` 
* $\alpha$ prune 
* maxdepth prune
* prune or not
* gini / entropy (split) 
* \# trees / \# BS samples 
* grid search etc. for tuning
* value(s) of `mtry` 
* OOB vs CV for tuning
:::

::::

## Bias-Variance Tradeoff

```{r fig.cap = "Test and training error as a function of model complexity.  Note that the error goes down monotonically only for the training data.  Be careful not to overfit!!  image credit: ISLR", out.width='90%', fig.align='center', echo=FALSE}
knitr::include_graphics("../images/varbias.png")
```



## Reflecting on Model Building

```{r echo = FALSE, fig.cap = "Image credit: https://www.tmwr.org/"}
knitr::include_graphics("../images/modelbuild1.png")
```



## Reflecting on Model Building

```{r echo = FALSE, fig.cap = "Image credit: https://www.tmwr.org/"}
knitr::include_graphics("../images/modelbuild2.png")
```





## Reflecting on Model Building

```{r echo = FALSE, fig.cap = "Image credit: https://www.tmwr.org/", out.width = "70%", fig.align='center'}
knitr::include_graphics("../images/modelbuild3.png")
```


