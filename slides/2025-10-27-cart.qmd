---
title: "Decision Trees"
author: "Jo Hardin"
subtitle: "October 27, 2025"
format:
  revealjs:
    incremental: false
    scrollable: true
    slide-number: true
    show-slide-number: all
    embed-resources: true
    html-math-method: mathjax
execute:
  echo: true
  warning: false
  message: false
bibliography: 
  - ../slides.bib
---


```{r include=FALSE}
library(tidyverse)
library(mosaic)
library(boot)
library(skimr)
options(pillar.width = 70)
```

# Agenda 10/27/25

1. Decision Trees



## `tidymodels` syntax

1. partition the data
2. build a recipe
3. select a model
4. create a workflow
5. fit the model  
6. validate the model



## Decision trees in action



```{r fig.cap = "A visual introduction to machine learning by Yee and Chu.", out.width='100%', fig.align='center', echo=FALSE}
knitr::include_graphics("../images/sfnyc.png")
```

Yee and  Chu created a step-by-step build of a recursive binary tree to model the differences between homes in SF and homes in NYC.  <a href = "http://www.r2d3.us/visual-intro-to-machine-learning-part-1/" target = "_blank">http://www.r2d3.us/visual-intro-to-machine-learning-part-1/</a>


## Classification and Regression Trees (CART)

**Basic Classification and Regression Trees (CART) Algorithm:**

1. Start with all observations in one group.
2. Find the variable/split that best separates the response variable (successive binary partitions based on the different predictors / explanatory variables).
    * Evaluation "homogeneity" within each group
    * Divide the data into two groups ("leaves") on that split ("node").
    * Within each split, find the best variable/split that separates the outcomes.
3. Continue until the groups are too small or sufficiently "pure".
4. Prune tree.



## Minimize heterogeneity

For every observation that falls into the region $R_m$, 

prediction = the mean of the response values for observations in $R_m$.  

$\Rightarrow$ Minimize Residual Sum of Squares (RSS): 
$$RSS = \sum_{m=1}^{|T|} \sum_{i \in R_m} (y_i - \overline{y}_{R_m})^2$$
where $\overline{y}_{R_m}$ is the mean response for observations within the $m$th region.



## Recursive binary splitting

Select the predictor $X_j$ and the cutpoint $s$ such that splitting the predictor space into the regions $\{X | X_j< s\}$ and $\{X | X_j \geq s\}$ lead to the greatest reduction in RSS.

For any $j$ and $s$, define the pair of half-planes to be
$$R_1(j,s) = \{X | X_j < s\} \mbox{ and } R_2(j,s) = \{X | X_j \geq s\}$$
Find the value of $j$ and $s$ that minimize the equation:
$$\sum_{i:x_i \in R_1(j,s)} (y_i - \overline{y}_{R_1})^2 + \sum_{i:x_i \in R_2(j,s)} (y_i - \overline{y}_{R_2})^2$$

where $\overline{y}_{R_1}$ is the mean response for observations in $R_1(j,s)$ and $\overline{y}_{R_2}$ is the mean response observations in $R_2(j,s)$.




##  Trees in action

```{r fig.cap = "", out.width='100%', fig.align='center', echo=FALSE}
#| eval: true
knitr::include_graphics("../images/decisiontrees.gif")
```




## Measures of impurity

$\hat{p}_{mk}$ = proportion of observations in the $m$th region from the $k$th class. 

* *classification error rate* = fraction of observations in the node & not in the most common class: 

$$E_m = 1 - \max_k(\hat{p}_{mk})$$
 
* *Gini index* 
$$G_m= \sum_{k=1}^K \hat{p}_{mk}(1-\hat{p}_{mk})$$

* *cross-entropy* 
$$D_m = - \sum_{k=1}^K \hat{p}_{mk} \log \hat{p}_{mk}$$
(Gini index & cross-entropy will both take on a value near zero if the $\hat{p}_{mk}$ values are all near zero or all near one.)



## Recursive binary splitting

For any $j$ and $s$, define the pair of half-planes to be
$$R_1(j,s) = \{X | X_j < s\} \mbox{ and } R_2(j,s) = \{X | X_j \geq s\}$$

Seek the value of $j$ and $s$ that minimize the equation:
\begin{align}
& \sum_{i:x_i \in R_1(j,s)} \sum_{k=1}^K \hat{p}_{{R_1}k}(1-\hat{p}_{{R_1}k}) + \sum_{i:x_i \in R_2(j,s)} \sum_{k=1}^K \hat{p}_{{R_2}k}(1-\hat{p}_{{R_2}k})\\
\\
\mbox{equivalently:    } & \\
& n_{R_1} \sum_{k=1}^K \hat{p}_{{R_1}k}(1-\hat{p}_{{R_1}k}) + n_{R_2} \sum_{k=1}^K \hat{p}_{{R_2}k}(1-\hat{p}_{{R_2}k})\\
\end{align}



## Stopping

We can always make the tree more "pure" by continuing the split.

> Too many splits will overfit the model to the training data!

Ways to control:

* `cost_complexity`
* `tree_depth`
* `min_n`

Overfitting: <a href = "http://www.r2d3.us/visual-intro-to-machine-learning-part-2/" target = "_blank">http://www.r2d3.us/visual-intro-to-machine-learning-part-2/</a>


# Agenda 10/29/25

1. pruning trees
2. cross validation to find $\alpha$ 

## Cost complexity

There is a cost to having a larger (more complex!) tree. 

Define the cost complexity criterion, $\alpha > 0:$
\begin{align}
\mbox{numerical: } C_\alpha(T) &= \sum_{m=1}^{|T|} \sum_{i \in R_m} (y_i - \overline{y}_{R_m})^2 + \alpha \cdot |T|\\
\mbox{categorical: } C_\alpha(T) &= \sum_{m=1}^{|T|} \sum_{i \in R_m} I(y_i \ne k(m)) + \alpha \cdot |T|
\end{align}
where $k(m)$ is the class with the majority of observations in node $m$ and $|T|$ is the number of terminal nodes in the tree.


* $\alpha$ small:  If $\alpha$ is set to be small, we are saying that the risk is more worrisome than the complexity and larger trees are favored because they reduce the risk.
* $\alpha$ large:  If $\alpha$ is set to be large, then the complexity of the tree is more worrisome and smaller trees are favored.



## In practice

Consider $\alpha$ increasing.  As $\alpha$ gets bigger, the "best" tree will be smaller.  

The [training error]{.underline} will be monotonically related to the size of the training tree.

However, the [test error]{.underline} will **not** be monotonically related to the size of the training tree.


```{r  out.width='80%', fig.align='center', echo=FALSE}
#| eval: false
knitr::include_graphics("../images/treealpha.jpg")
```


## A note on $\alpha$

In the text (*Introduction to Statistical Learning*) and almost everywhere else you might look, the cost complexity is defined as in previous slides.

However, you might notice that in R the `cost_complexity` value is typically less than 1.  The value of the function that is being minimized in R is the **average** of the squared errors and the missclassification **rate**. (Allows for $\alpha$ to not depend on sample size.)

\begin{align}
\mbox{numerical: } C_\alpha(T) &= \frac{1}{n}\sum_{m=1}^{|T|} \sum_{i \in R_m} (y_i - \overline{y}_{R_m})^2 + \alpha \cdot |T|\\
\mbox{categorical: } C_\alpha(T) &= \frac{1}{n}\sum_{m=1}^{|T|} \sum_{i \in R_m} I(y_i \ne k(m)) + \alpha \cdot |T|
\end{align}



## CART algorithm

**Algorithm**:  Building a Regression Tree

1.  Use recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations.
2.  Apply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of $\alpha$.
3. Use $V$-fold cross-validation to choose $\alpha$.  Divide the training observations into $V$ folds, then for each $v=1, 2, \ldots, V$:
    a. Repeat Steps 1 and 2 on all but the $v$th fold of the training data.
    b. Evaluate the mean squared prediction error on the data in the left-out $k$th fold, as a function of $\alpha$.
    For each value of $\alpha$, average the prediction error (either misclassification or RSS), and pick $\alpha$ to minimize the average error.
4. Return the subtree from Step 2 that corresponds to the chosen value of $\alpha$.

******



## CART example w defaults

```{r echo = FALSE}
library(tidymodels)
library(palmerpenguins)

set.seed(47)
penguin_split <- initial_split(penguins)
penguin_train <- training(penguin_split)
penguin_test <- testing(penguin_split)
```


::: {.panel-tabset}

## recipe
```{r}
#| message: true

penguin_cart_recipe <-
  recipe(species ~ . ,
         data = penguin_train) |>
  step_unknown(sex, new_level = "unknown") |>
  step_mutate(year = as.factor(year)) 

penguin_cart_recipe
```

## model
```{r}
penguin_cart <- decision_tree() |>
  set_engine("rpart") |>
  set_mode("classification")

penguin_cart
```

## workflow
```{r}
penguin_cart_wflow <- workflow() |>
  add_model(penguin_cart) |>
  add_recipe(penguin_cart_recipe)

penguin_cart_wflow
```

## fit
```{r}
penguin_cart_fit <- penguin_cart_wflow |>
  fit(data = penguin_train)

penguin_cart_fit 
```

## pred
```{r}
penguin_cart_fit |>
  predict(new_data = penguin_train) |>
  cbind(penguin_train) |>
  select(.pred_class, species) |>
  table()
```

Not quite ready for test data because $\alpha$ is unknown.
:::



##  Plotting the tree (not tidy)


::::: {.panel-tabset}

## plot 1

:::: {.columns}

::: {.column width=50%}
```{r eval = FALSE}
library(rpart.plot)
penguins_cart_plot <- 
  penguin_cart_fit |>
  extract_fit_parsnip()

rpart.plot(
  penguins_cart_plot$fit,
  roundint = FALSE)  
```
:::

::: {.column width=50%}
```{r echo = FALSE}
library(rpart.plot)
penguins_cart_plot <- 
  penguin_cart_fit |>
  extract_fit_parsnip()

rpart.plot(
  penguins_cart_plot$fit,
  roundint = FALSE)  
```
:::
::::

## plot 2
:::: {.columns}

::: {.column width=50%}

```{r eval = FALSE}
library(rattle)
penguins_cart_plot <- 
  penguin_cart_fit |>
  extract_fit_parsnip()

fancyRpartPlot(
  penguins_cart_plot$fit,
  sub = NULL,
  palettes = "RdPu")
```
:::

::: {.column width=50%}
```{r echo = FALSE}
library(rattle)
penguins_cart_plot <- 
  penguin_cart_fit |>
  extract_fit_parsnip()

fancyRpartPlot(
  penguins_cart_plot$fit,
  sub = NULL,
  palettes = "RdPu")  # change the color 
```

:::

::::

:::::


## CART example w CV


::: {.panel-tabset}

## new recipe
```{r}
penguin_cart_tune_recipe <-
  recipe(sex ~ body_mass_g + bill_length_mm + species,
         data = penguin_train)  
```

## creating folds
```{r}
set.seed(470)
penguin_vfold <- vfold_cv(penguin_train,
                          v = 5, strata = sex)
```

## alpha
```{r}
cart_grid <- expand.grid(
  cost_complexity = c(0, 10^(seq(-5,-1,1))),
  tree_depth = seq(1,6, by = 1))

cart_grid
```

## tune wkflow
```{r}
penguin_cart_tune <- 
  decision_tree(cost_complexity = tune(),
                tree_depth = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")

penguin_cart_wflow_tune <- workflow() |>
  add_model(penguin_cart_tune) |>
  add_recipe(penguin_cart_tune_recipe)
```

## tuning
```{r}
penguin_tuned <- penguin_cart_wflow_tune |>
  tune_grid(resamples = penguin_vfold, 
           grid = cart_grid) 

penguin_tuned |> collect_metrics() |>
  filter(.metric == "accuracy") |> 
  arrange(desc(mean))
```

:::




##  Parameter choice

(Hard to see because `tree_depth = 6` is on top of `tree_depth = 5`, either would be fine to choose.)

```{r fig.height = 3.5, fig.align='center'}
penguin_tuned |>
  collect_metrics() |>
  filter(.metric == "accuracy") |> 
  ggplot(aes(x = cost_complexity, y = mean, 
             color = as.factor(tree_depth))) + 
  geom_line()

penguin_tuned |> 
  select_best(metric = "accuracy")
```



## Best model -- using CV params

```{r}
penguin_opt <- decision_tree(cost_complexity = 0, 
                             tree_depth = 5) |>
  set_engine("rpart") |>
  set_mode("classification")

penguin_opt

penguin_final_opt <-
  workflow() |>
  add_model(penguin_opt) |>
  add_recipe(penguin_cart_tune_recipe) |>
  fit(data = penguin_train)
```

<!--
# Best model -- using `finalize_model()`

```{r}
penguin_best <- finalize_model(
  penguin_cart_tune,
  select_best(penguin_tuned, metric = "accuracy"))

workflow() |>
  add_model(penguin_best) |>
  add_recipe(penguin_cart_tune_recipe) |>
  fit(data = penguin_train)
```

-->

## Best Model Predictions

<!--
You get the same model whether you use the CV parameters that you found or whether you use `finalize_model()` to pull out the CV parameters.


::: {.panel-tabset}

## using CV params
-->

```{r}
workflow() |>
  add_model(penguin_opt) |>
  add_recipe(penguin_cart_tune_recipe) |>
  fit(data = penguin_train) |>
  predict(new_data = penguin_test) |>
  cbind(penguin_test) |>
  select(.pred_class, sex) |>
  table()
```

<!--
## using `finalize_model()`

```{r}
workflow() |>
  add_model(penguin_best) |>
  add_recipe(penguin_cart_tune_recipe) |>
  fit(data = penguin_train) |>
  predict(new_data = penguin_test) |>
  cbind(penguin_test) |>
  select(.pred_class, sex) |>
  table()
```

:::
-->

## Best Model Predictions

:::: {.columns}

::: {.column width=50%}
```{r eval = FALSE}
library(rpart.plot)
penguins_cart_plot <- 
workflow() |>
  add_model(penguin_opt) |>
  add_recipe(penguin_cart_tune_recipe) |>
  fit(data = penguin_train) |>
  extract_fit_parsnip()

rpart.plot(
  penguins_cart_plot$fit,
  roundint = FALSE) 
```
:::

::: {.column width=50%}
```{r echo = FALSE}
library(rpart.plot)
penguins_cart_plot <- 
workflow() |>
  add_model(penguin_opt) |>
  add_recipe(penguin_cart_tune_recipe) |>
  fit(data = penguin_train) |>
  extract_fit_parsnip()

rpart.plot(
  penguins_cart_plot$fit,
  roundint = FALSE) 
```

:::

::::


## Bias-Variance Tradeoff


* **Variance** refers to the amount by which would change if we estimated it using a different training set. Generally, the [closer the model fits the (training) data]{.underline}, the more variable it will be (it will be different for each data set!). A model with many many explanatory variables will often fit the data too closely.

* **Bias** refers to the error that is introduced by approximating the "truth" by a model which is [too simple]{.underline}. For example, we often use linear models to describe complex relationships, but it is unlikely that any real life situation actually has a true linear model. However, if the true relationship is close to linear, then the linear model will have a low bias.

* an excellent resource for explaining the bias-variance trade-off: <a href = "http://scott.fortmann-roe.com/docs/BiasVariance.html" target = "_blank">http://scott.fortmann-roe.com/docs/BiasVariance.html</a>

## Bias-Variance Tradeoff

| model | high bias <br> (too simple) | high variance <br> (overfit training data) |
| ----- | --------- | ------------- |
| linear model | one (zero!) predictor | many many predictors |
| $k$-NN | $k = n$ | $k = 1$ |
| CART | one split | single observation in every terminal node |

## Bias-Variance Tradeoff

```{r fig.cap = "Test and training error as a function of model complexity.  Note that the error goes down monotonically only for the training data.  Be careful not to overfit!!  image credit: ISLR", out.width='90%', fig.align='center', echo=FALSE}
knitr::include_graphics("../images/varbias.png")
```



## Reflecting on Model Building

```{r echo = FALSE, fig.cap = "Image credit: https://www.tmwr.org/"}
knitr::include_graphics("../images/modelbuild1.png")
```



## Reflecting on Model Building

```{r echo = FALSE, fig.cap = "Image credit: https://www.tmwr.org/"}
knitr::include_graphics("../images/modelbuild2.png")
```



## Reflecting on Model Building

```{r echo = FALSE, fig.cap = "Image credit: https://www.tmwr.org/", out.width = "70%", fig.align='center'}
knitr::include_graphics("../images/modelbuild3.png")
```

# Bagging preview

## Why a forest instead of a tree?

* Trees suffer from very **high variance**

* Two datasets will provide extremely different trees

* To reduce variability, fit many trees and aggregate the predictions.

* No longer reporting the tree structure, now reporting the predicted values (with hopes that predictions from one dataset will be similar to predictions from another dataset).


## Bagging

Bagging = **B**ootstrap **Agg**regating. 

* multiple models aggregate together will produce a smoother model fit and better balance between 
   - bias in the fit
   - variance in the fit.  
   
Bagging can be applied to any classifier to reduce variability.

<p style = "color:purple">Recall that the variance of the sample mean is variance of data / n.  So we've seen the idea that averaging an outcome gives reduced variability.</p>



## Bagging Models

```{r fig.cap = "Image credit: https://www.pluralsight.com/guides/ensemble-methods:-bagging-versus-boosting", out.width='70%', fig.align='center', echo=FALSE}
knitr::include_graphics("../images/bagging.png")
```



## Bagging Trees

```{r fig.cap = "Image credit: https://www.analyticsvidhya.com/blog/2021/05/bagging-25-questions-to-test-your-skills-on-random-forest-algorithm/", out.width='90%', fig.align='center', echo=FALSE}
knitr::include_graphics("../images/baggedtree.png")
```



## Algorithm:  Bagging Forest
1. Resample (bootstrap) *cases* (observational units, not variables).  
2. Build a tree on each new set of (bootstrapped) training observations.
3. Average (regression) or majority vote (classification).
4. Note that for every bootstrap sample, approximately 2/3 of the observations will be chosen and 1/3 of them will not be chosen.


## Observations not in a given tree


P(observation $i$ is not in the bootstrap sample) = $\bigg(1 - \frac{1}{n} \bigg)^n$


$$\lim_{n \rightarrow \infty} \bigg(1 - \frac{1}{n} \bigg)^n = \frac{1}{e} \approx \frac{1}{3}$$



## OOB Error

**Out of Bag**

with bagging, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally **for free**!!!

* Each tree is constructed using a different bootstrap sample 

* Put all of the cases left out in the construction of the $b^{th}$ tree down the $b^{th}$ tree to get a classification. 

* At the end of the run, 
   - take $j$ to be the class that got most of the votes every time case $i$ was oob. 
   - The proportion of times that $j$ is not equal to the true class averaged over all cases is the **oob error**. 

## OOB Prediction {.smaller}

Let the OOB prediction for the $i^{th}$ observation to be  $\hat{y}_{(-i)}$

\begin{eqnarray*}
\mbox{OOB}_{\mbox{error}} &=& \frac{1}{n} \sum_{i=1}^n \textrm{I} (y_i \ne \hat{y}_{(-i)}) \ \ \ \ \ \ \ \  \mbox{classification}\\
\mbox{OOB}_{\mbox{error}} &=& \frac{1}{n} \sum_{i=1}^n  (y_i - \hat{y}_{(-i)})^2  \ \ \ \ \ \ \ \ \mbox{regression}\\
\end{eqnarray*}

| obs | tree1 | tree2 | tree3 | tree4 | $\cdots$ | tree100 | average           |
|-----|:-----:|:-----:|:-----:|:-----:|:--------:|:-------:|--------------------|
|  1  |       |   X   |   X   |       |          |         | $\sum(pred)/38$    |
|  2  |   X   |       |       |       |          |         | $\sum(pred)/30$    |
|  3  |       |       |       |   X   |          |   X     | $\sum(pred)/33$    |
|  4  |   X   |       |       |       |          |         | $\sum(pred)/32$    |
|  5  |   X   |       |       |       |          |         | $\sum(pred)/39$    |
|  6  |       |       |   X   |       |          |   X     | $\sum(pred)/29$    |
|  7  |       |       |       |       |          |   X     | $\sum(pred)/29$    |
|  8  |       |       |   X   |   X   |          |   X     | $\sum(pred)/31$    |
|  9  |       |       |       |   X   |          |         | $\sum(pred)/36$    |



## Bagging example w defaults

```{r echo = FALSE}
library(tidymodels)
library(palmerpenguins)

set.seed(47)
penguin_split <- initial_split(penguins)
penguin_train <- training(penguin_split)
penguin_test <- testing(penguin_split)
```

::::: {.panel-tabset}

## recipe
```{r}
#| message: true
penguin_bag_recipe <-
  recipe(body_mass_g ~ . ,
         data = penguin_train) |>
  step_unknown(sex, new_level = "unknown") |>
  step_mutate(year = as.factor(year)) |>
  step_naomit(all_predictors(), body_mass_g)

penguin_bag_recipe
```

## model
```{r}
num_trees <- 500
penguin_bag <- rand_forest(mtry = 7,
                           trees = num_trees) |>
  set_engine("randomForest", oob.error = TRUE) |>
  set_mode("regression")

penguin_bag
```

## workflow
```{r}
penguin_bag_wflow <- workflow() |>
  add_model(penguin_bag) |>
  add_recipe(penguin_bag_recipe)

penguin_bag_wflow
```

## fit


:::: {.columns}
::: {.column width=50%}
```{r eval = FALSE}
penguin_bag_fit <- 
  penguin_bag_wflow |>
  fit(data = penguin_train)

penguin_bag_fit 
```
:::

::: {.column width=50%}
```{r echo = FALSE}
penguin_bag_fit <- 
  penguin_bag_wflow |>
  fit(data = penguin_train)

penguin_bag_fit 
```
:::

::::

:::::


## Fit again
```{r echo = FALSE}
#| message: true
penguin_bag_fit <- 
  penguin_bag_wflow |>
  fit(data = penguin_train)

penguin_bag_fit 
```



##  Plotting the OOB (not tidy)


::::: {.panel-tabset}

## plot 1

:::: {.columns}

::: {.column width=50%}
```{r eval = FALSE}
rsq <- penguin_bag_fit |>
  extract_fit_parsnip() |>
  pluck("fit") |>
  pluck("rsq")


data.frame(
  trees = 1:num_trees, 
  rsq = rsq) |>
  ggplot() + 
  geom_line(
    aes(x = trees, y = rsq)) +
  ylab("") + 
  xlab("Number of Trees") + 
  ggtitle("OOB R-squared")
```
:::

::: {.column width=50%}
```{r echo = FALSE}
rsq <- penguin_bag_fit |>
  extract_fit_parsnip() |>
  pluck("fit") |>
  pluck("rsq")  # or mse

data.frame(trees = 1:num_trees, rsq = rsq) |>
  ggplot() + 
  geom_line(aes(x = trees, y = rsq)) +
  ylab("") + xlab("Number of Trees") + 
  ggtitle("OOB R-squared")
```
:::

::::

## plot 2
:::: {.columns}

::: {.column width=50%}
```{r eval = FALSE}
MSE <- penguin_bag_fit |>
  extract_fit_parsnip() |>
  pluck("fit") |>
  pluck("mse")

data.frame(
  trees = 1:num_trees, 
  MSE = MSE) |>
  ggplot() + 
  geom_line(
    aes(x = trees, y = MSE)) +
  ylab("") + 
  xlab("Number of Trees") + 
  ggtitle("OOB MSE")
```
:::

::: {.column width=50%}
```{r echo = FALSE}
MSE <- penguin_bag_fit |>
  extract_fit_parsnip() |>
  pluck("fit") |>
  pluck("mse")  

data.frame(
  trees = 1:num_trees, 
  MSE = MSE) |>
  ggplot() + 
  geom_line(aes(x = trees, y = MSE)) +
  ylab("") + xlab("Number of Trees") + 
  ggtitle("OOB MSE")
```

:::

::::

:::::



