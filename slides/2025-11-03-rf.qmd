---
title: "Bagging and Random Forests"
author: "Jo Hardin"
subtitle: "November 4 + 6, 2024"
format:
  revealjs:
    incremental: false
    scrollable: true
    slide-number: true
    show-slide-number: all
    embed-resources: true
    html-math-method: mathjax
execute:
  echo: true
  warning: false
  message: false
bibliography: 
  - ../slides.bib
---


```{r include=FALSE}
library(tidyverse)
library(mosaic)
library(boot)
library(skimr)
```

# Agenda 11/04/2024

1. Redux - CART
2. bagging process
3. bagging error rate (OOB error)



## `tidymodels` syntax

1. partition the data
2. build a recipe
3. select a model
4. create a workflow
5. fit the model  
6. validate the model



## Random Forests

> Random forests are an ensemble learning method for classification (and regression) that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes output by individual trees -- [Wikipedia](http://en.wikipedia.org/wiki/Random_forest)



## Why a forest instead of a tree?

* Trees suffer from very **high variance**

* Two datasets will provide extremely different trees

* To reduce variability, fit many trees and aggregate the predictions.

* No longer reporting the tree structure, now reporting the predicted values (with hopes that predictions from one dataset will be similar to predictions from another dataset).


## Bagging

Bagging = **B**ootstrap **Agg**regating. 

* multiple models aggregate together will produce a smoother model fit and better balance between 
   - bias in the fit
   - variance in the fit.  
   
Bagging can be applied to any classifier to reduce variability.

<p style = "color:purple">Recall that the variance of the sample mean is variance of data / n.  So we've seen the idea that averaging an outcome gives reduced variability.</p>



## Bagging Models

```{r fig.cap = "Image credit: https://www.pluralsight.com/guides/ensemble-methods:-bagging-versus-boosting", out.width='70%', fig.align='center', echo=FALSE}
knitr::include_graphics("../images/bagging.png")
```



## Bagging Trees

```{r fig.cap = "Image credit: https://www.analyticsvidhya.com/blog/2021/05/bagging-25-questions-to-test-your-skills-on-random-forest-algorithm/", out.width='90%', fig.align='center', echo=FALSE}
knitr::include_graphics("../images/baggedtree.png")
```



## Algorithm:  Bagging Forest
1. Resample (bootstrap) *cases* (observational units, not variables).  
2. Build a tree on each new set of (bootstrapped) training observations.
3. Average (regression) or majority vote (classification).
4. Note that for every bootstrap sample, approximately 2/3 of the observations will be chosen and 1/3 of them will not be chosen.


## Observations not in a given tree


P(observation $i$ is not in the bootstrap sample) = $\bigg(1 - \frac{1}{n} \bigg)^n$


$$\lim_{n \rightarrow \infty} \bigg(1 - \frac{1}{n} \bigg)^n = \frac{1}{e} \approx \frac{1}{3}$$



## OOB Error

**Out of Bag**

with bagging, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally **for free**!!!

* Each tree is constructed using a different bootstrap sample 

* Put all of the cases left out in the construction of the $b^{th}$ tree down the $b^{th}$ tree to get a classification. 

* At the end of the run, 
   - take $j$ to be the class that got most of the votes every time case $i$ was oob. 
   - The proportion of times that $j$ is not equal to the true class averaged over all cases is the **oob error**. 

## OOB Prediction {.smaller}

Let the OOB prediction for the $i^{th}$ observation to be  $\hat{y}_{(-i)}$

\begin{eqnarray*}
\mbox{OOB}_{\mbox{error}} &=& \frac{1}{n} \sum_{i=1}^n \textrm{I} (y_i \ne \hat{y}_{(-i)}) \ \ \ \ \ \ \ \  \mbox{classification}\\
\mbox{OOB}_{\mbox{error}} &=& \frac{1}{n} \sum_{i=1}^n  (y_i - \hat{y}_{(-i)})^2  \ \ \ \ \ \ \ \ \mbox{regression}\\
\end{eqnarray*}

| obs | tree1 | tree2 | tree3 | tree4 | $\cdots$ | tree100 | average           |
|-----|:-----:|:-----:|:-----:|:-----:|:--------:|:-------:|--------------------|
|  1  |       |   X   |   X   |       |          |         | $\sum(pred)/38$    |
|  2  |   X   |       |       |       |          |         | $\sum(pred)/30$    |
|  3  |       |       |       |   X   |          |   X     | $\sum(pred)/33$    |
|  4  |   X   |       |       |       |          |         | $\sum(pred)/32$    |
|  5  |   X   |       |       |       |          |         | $\sum(pred)/39$    |
|  6  |       |       |   X   |       |          |   X     | $\sum(pred)/29$    |
|  7  |       |       |       |       |          |   X     | $\sum(pred)/29$    |
|  8  |       |       |   X   |   X   |          |   X     | $\sum(pred)/31$    |
|  9  |       |       |       |   X   |          |         | $\sum(pred)/36$    |




## Bagging example w defaults

```{r echo = FALSE}
library(tidymodels)
library(palmerpenguins)

set.seed(47)
penguin_split <- initial_split(penguins)
penguin_train <- training(penguin_split)
penguin_test <- testing(penguin_split)
```

::::: {.panel-tabset}

## recipe
```{r}
#| message: true
penguin_bag_recipe <-
  recipe(body_mass_g ~ . ,
         data = penguin_train) |>
  step_unknown(sex, new_level = "unknown") |>
  step_mutate(year = as.factor(year)) |>
  step_naomit(all_predictors(), body_mass_g)

penguin_bag_recipe
```

## model
```{r}
num_trees <- 500
penguin_bag <- rand_forest(mtry = 7,
                           trees = num_trees) |>
  set_engine("randomForest", oob.error = TRUE) |>
  set_mode("regression")

penguin_bag
```

## workflow
```{r}
penguin_bag_wflow <- workflow() |>
  add_model(penguin_bag) |>
  add_recipe(penguin_bag_recipe)

penguin_bag_wflow
```

## fit


:::: {.columns}
::: {.column width=50%}
```{r eval = FALSE}
penguin_bag_fit <- 
  penguin_bag_wflow |>
  fit(data = penguin_train)

penguin_bag_fit 
```
:::

::: {.column width=50%}
```{r echo = FALSE}
penguin_bag_fit <- 
  penguin_bag_wflow |>
  fit(data = penguin_train)

penguin_bag_fit 
```
:::

::::

:::::


## Fit again
```{r echo = FALSE}
#| message: true
penguin_bag_fit <- 
  penguin_bag_wflow |>
  fit(data = penguin_train)

penguin_bag_fit 
```



##  Plotting the OOB (not tidy)


::::: {.panel-tabset}

## plot 1

:::: {.columns}

::: {.column width=50%}
```{r eval = FALSE}
rsq <- penguin_bag_fit |>
  extract_fit_parsnip() |>
  pluck("fit") |>
  pluck("rsq")


data.frame(
  trees = 1:num_trees, 
  rsq = rsq) |>
  ggplot() + 
  geom_line(
    aes(x = trees, y = rsq)) +
  ylab("") + 
  xlab("Number of Trees") + 
  ggtitle("OOB R-squared")
```
:::

::: {.column width=50%}
```{r echo = FALSE}
rsq <- penguin_bag_fit |>
  extract_fit_parsnip() |>
  pluck("fit") |>
  pluck("rsq")  # or mse

data.frame(trees = 1:num_trees, rsq = rsq) |>
  ggplot() + 
  geom_line(aes(x = trees, y = rsq)) +
  ylab("") + xlab("Number of Trees") + 
  ggtitle("OOB R-squared")
```
:::

::::

## plot 2
:::: {.columns}

::: {.column width=50%}
```{r eval = FALSE}
MSE <- penguin_bag_fit |>
  extract_fit_parsnip() |>
  pluck("fit") |>
  pluck("mse")

data.frame(
  trees = 1:num_trees, 
  MSE = MSE) |>
  ggplot() + 
  geom_line(
    aes(x = trees, y = MSE)) +
  ylab("") + 
  xlab("Number of Trees") + 
  ggtitle("OOB MSE")
```
:::

::: {.column width=50%}
```{r echo = FALSE}
MSE <- penguin_bag_fit |>
  extract_fit_parsnip() |>
  pluck("fit") |>
  pluck("mse")  

data.frame(
  trees = 1:num_trees, 
  MSE = MSE) |>
  ggplot() + 
  geom_line(aes(x = trees, y = MSE)) +
  ylab("") + xlab("Number of Trees") + 
  ggtitle("OOB MSE")
```

:::

::::

:::::



# Agenda 11/06/24

1. Random Forests
2. Example


## Random Forests

* Many trees
  - bootstrap the data before building each tree
  - for best split look only at a random $m$ variables (different $m$ variables per split)
  
* Tuning:  look at OOB predictions -- either average or majority vote

* Testing:  test data will be (average or majority vote) prediction for all the trees.





## Algorithm:  Random Forest

1. Bootstrap sample from the training set.
2. Grow an un-pruned tree on the bootstrap sample.
* *At each split*, select $m$ variables and determine the best split using only the $m$ predictors.
* Do *not* prune the tree.  Save the tree as is!
3. Repeat steps 1-2 for many many trees.
4. For each tree grown on a bootstrap sample, predict the OOB samples.  For each tree grown, $~1/3$ of the training samples won't be in the bootstrap sample -- those are called out of bootstrap (OOB) samples.  OOB samples can be used as *test* data to estimate the error rate of the entire forest.
5. Combine the OOB predictions to create the "out-of-bag" error rate (either majority vote or average of predictions / class probabilities).
6. All trees together represent the *model* that is used for new predictions (either majority vote or average).


## Overall misclassification rate

The error rate (for classification forest) is: $$\frac{\# \mbox{ misclassificiations}}{n}$$

* The prediction of an observation come in two steps.  First, an observation is run down a tree until it gets to a terminal node.  The majority class in that terminal node (from the training observations) provides the class for the prediction from that tree.  Second, the observation goes down the relevant trees, each providing a class, and the final prediction of the observation is given by the majority class across the trees.

* Each observation will get one prediction.  If the predicted class is different from the observed class, the point is considered to be misclassified.

* Keep in mind that the trees and forests that will be used are different depending on whether the error rate is calculated under OOB, CV, training, or testing scenarios.



## Variable Importance

> importance = decrease in node impurity resulting from splits over that variable, averaged over all trees.


* For each tree, the prediction error on the out-of-bag portion of the data is recorded (error rate for classification, MSE for regression).  

* Keep the set of trees, and **permute** the $j^{th}$ variable and recalculate the oob prediction error.  

* The difference between the two oob errors are averaged over all trees (for the $j^{th}$ variable) to give the importance for the $j^{th}$ variable.




## RF technical details

* unfortunately, **tidymodels** currently does not have an OOB option for model tuning (e.g., to find `mtry`)

* unlike `num_trees`, `mtry` is not calculated "as you go along"

* cross validation will be used to find the optimal value of `mtry`



## RF example w CV tuning

```{r echo = FALSE}
library(tidymodels)
library(palmerpenguins)

penguins <- penguins |>
  drop_na()

set.seed(47)
penguin_split <- initial_split(penguins)
penguin_train <- training(penguin_split)
penguin_test <- testing(penguin_split)
```

::: {.panel-tabset}

## recipe
```{r}
#| message: true
penguin_rf_recipe <-
  recipe(body_mass_g ~ . ,
         data = penguin_train) |>
  step_unknown(sex, new_level = "unknown") |>
  step_mutate(year = as.factor(year)) 

penguin_rf_recipe
```

## model
```{r}
penguin_rf <- rand_forest(mtry = tune(),
                           trees = tune()) |>
  set_engine("ranger", importance = "permutation") |>
  set_mode("regression")

penguin_rf
```

## workflow

```{r}
penguin_rf_wflow <- workflow() |>
  add_model(penguin_rf) |>
  add_recipe(penguin_rf_recipe)

penguin_rf_wflow
```


## CV
```{r}
set.seed(234)
penguin_folds <- vfold_cv(penguin_train,
                          v = 4)
```

## param
```{r}
penguin_grid <- grid_regular(mtry(range = c(2,7)),
                             trees(range = c(1,500)), levels = 5)

penguin_grid
```

## tune
```{r}
penguin_rf_tune <- 
  penguin_rf_wflow |>
  tune_grid(resamples = penguin_folds,
            grid = penguin_grid)

penguin_rf_tune 
```
:::


##  RF model output

```{r fig.height = 5}
penguin_rf_tune |>
  collect_metrics() |>
  filter(.metric == "rmse") |>
  mutate(mtry = as.factor(mtry)) |> 
  ggplot() + 
  geom_line(aes(x = trees, y = mean, color = mtry)) + 
  labs(title = "RMSE: root mean squared error", y = "")
```





## RF Final model

```{r}
penguin_rf_best <- finalize_model(
  penguin_rf,
  select_best(penguin_rf_tune, metric = "rmse"))

penguin_rf_best


penguin_rf_final <-
  workflow() |>
  add_model(penguin_rf_best) |>
  add_recipe(penguin_rf_recipe) |>
  fit(data = penguin_train)
```



## RF Final model

```{r}
penguin_rf_final
```



# VIP 

```{r fig.height = 5}
library(vip)

penguin_rf_final |>
  extract_fit_parsnip() |>
  vip(geom = "point")
```



## Test predictions

```{r fig.height = 5}
penguin_rf_final |>
  predict(new_data = penguin_test) |>
  cbind(penguin_test) |>
  ggplot() +
  geom_point(aes(x = body_mass_g, y = .pred)) + 
  geom_abline(intercept = 0, slope = 1)
```



## Model choices

:::: {.columns}

::: {.column width=60%}

* explanatory variable choice                 
* number of explanatory variables             
* functions/transformation of explanatory                       
* transformation of response                
* response: continuous vs. categorical              
* how missing data is dealt with              
* train/test split (`set.seed`)                
* train/test proportion             
* type of classification model     
* use of cost complexity / parameter   
* majority / average prob (tree error rate)         
* accuracy vs sensitivity vs specificity         
:::

::: {.column width=40%}

* $k$ ($k$-NN) 
* distance measure
* V (CV)
* CV `set.seed` 
* $\alpha$ prune 
* maxdepth prune
* prune or not
* gini / entropy (split) 
* \# trees / \# BS samples 
* grid search etc. for tuning
* value(s) of `mtry` 
* OOB vs CV for tuning
:::

::::

## Bias-Variance Tradeoff

```{r fig.cap = "Test and training error as a function of model complexity.  Note that the error goes down monotonically only for the training data.  Be careful not to overfit!!  image credit: ISLR", out.width='90%', fig.align='center', echo=FALSE}
knitr::include_graphics("../images/varbias.png")
```



## Reflecting on Model Building

```{r echo = FALSE, fig.cap = "Image credit: https://www.tmwr.org/"}
knitr::include_graphics("../images/modelbuild1.png")
```



## Reflecting on Model Building

```{r echo = FALSE, fig.cap = "Image credit: https://www.tmwr.org/"}
knitr::include_graphics("../images/modelbuild2.png")
```





## Reflecting on Model Building

```{r echo = FALSE, fig.cap = "Image credit: https://www.tmwr.org/", out.width = "70%", fig.align='center'}
knitr::include_graphics("../images/modelbuild3.png")
```


