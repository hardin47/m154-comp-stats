---
title: "Using recipes"
author: "Jo Hardin"
subtitle: "October 20, 2025"
format: 
  revealjs:
    incremental: false
    scrollable: true
    slide-number: true
    show-slide-number: all
    embed-resources: true
    html-math-method: mathjax
execute:
  echo: true
  warning: false
  message: false
bibliography: 
  - ../slides.bib
---


```{r include=FALSE}
library(tidyverse)
library(mosaic)
library(boot)
library(skimr)
options(pillar.width = 70)
```



# Agenda 10/20/25

1. Feature engineering: should the data be modified?
2. `tidymodels` syntax for recipes  
   a. recipe / feature engineering  
   b. model  
   c. workflow  
   d. fit  
   e. validate  



## Modeling

```{r}
library(tidymodels)
```


```{r echo = FALSE}
knitr::include_graphics("../images/process.png")
```



## Motivation

```{r echo = FALSE}
knitr::include_graphics("../images/garbage.png")
```



## `tidymodels` syntax

1. partition the data
2. build a recipe
3. select a model
4. create a workflow
5. fit the model  
6. (validate the model)




## partition the data

Put the testing data in your pocket (keep it secret from the model / R!!)

```{r echo = FALSE, fig.cap = "Image credit: Julia Silge"}
knitr::include_graphics("../images/testtrain.png")
```


## partition the data


```{r}
library(tidymodels)
library(palmerpenguins)

set.seed(47)
penguin_split <- initial_split(penguins)
penguin_train <- training(penguin_split)
penguin_test <- testing(penguin_split)
```


# build a recipe

1. Start the `recipe()`
2. Define the **variables** involved
3. Describe preprocessing **step-by-step**



## feature engineering

> feature engineering is the process of transforming raw data into features (variables) that are better predictors (for the model at hand) of the response variable

* create new variables (e.g., combine levels $\rightarrow$ from state to region)
* transform variable (e.g., log, arcsine)
* continuous variables $\rightarrow$ discrete or categorical (e.g., binning)
* numerical categorical data $\rightarrow$ factors / character strings (dummy or one hot encoding) (e.g., zipcode)
* time $\rightarrow$ discretized time
* missing values $\rightarrow$ imputed
* NA $\rightarrow$ level
* continuous variables $\rightarrow$ center & scale ("normalize")
* date $\rightarrow$ weekday vs. weekend






## `step_` functions

For more information: https://recipes.tidymodels.org/reference/index.html

```{r}
apropos("^step_")
```



## the data: penguins

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill", fig.align='right', out.width="30%"}
knitr::include_graphics("../images/penguins.png")
```

```{r}
penguins
```





## recipe

```{r}
penguin_recipe <-
  recipe(body_mass_g ~ species + island + bill_length_mm + 
           bill_depth_mm + flipper_length_mm + sex + year,
         data = penguin_train) |>
  step_mutate(year = as.factor(year)) |>
  step_unknown(sex, new_level = "unknown") |>
  step_relevel(sex, ref_level = "female") |>
  update_role(island, new_role = "id variable")
```


```{r}
#| message: true

penguin_recipe
```


## silly recipe


```{r}
penguin_recipe_silly <-
  recipe(body_mass_g ~ species + island + bill_length_mm + 
           bill_depth_mm + flipper_length_mm + sex + year,
         data = penguin_train) |>
  step_log(all_numeric()) 
```


```{r}
#| message: true

penguin_recipe_silly
```

## model

To specify a model:

1. pick a **model**
2. set the **mode** (regression vs classification, if needed)
3. set the **engine**

```{r}
penguin_lm <- linear_reg() |>
  set_engine("lm")
```

```{r}
penguin_lm
```


## engines

::: {.panel-tabset}

## k-NN
```{r}
show_engines("nearest_neighbor")
```

## trees
```{r}
show_engines("decision_tree")
```

## rf
```{r}
show_engines("rand_forest")
```


## svm

```{r}
show_engines("svm_poly")
show_engines("svm_rbf")
```


## lm

```{r}
show_engines("linear_reg")
```

:::

## workflow

```{r}
penguin_wflow <- workflow() |>
  add_model(penguin_lm) |>
  add_recipe(penguin_recipe)
```

```{r}
penguin_wflow
```




## fit


```{r}
penguin_fit <- penguin_wflow |>
  fit(data = penguin_train)
```


```{r}
penguin_fit |> tidy()
```


## entire process


::: {.panel-tabset}

## recipe
```{r}
#| message: true

penguin_recipe <-
  recipe(body_mass_g ~ species + island + bill_length_mm + 
           bill_depth_mm + flipper_length_mm + sex + year,
         data = penguin_train) |>
  step_mutate(year = as.factor(year)) |>
  step_unknown(sex, new_level = "unknown") |>
  step_relevel(sex, ref_level = "female") |>
  update_role(island, new_role = "id variable")

penguin_recipe
```

## model
```{r}
penguin_lm <- linear_reg() |>
  set_engine("lm")

penguin_lm
```

## workflow

```{r}
penguin_wflow <- workflow() |>
  add_model(penguin_lm) |>
  add_recipe(penguin_recipe)

penguin_wflow
```

## fit
```{r}
penguin_fit <- penguin_wflow |>
  fit(data = penguin_train)

penguin_fit |> tidy()
```

:::

## model parameters

* Some model parameters are tuned from the data (some aren't).
  - linear model coefficients are optimized (not tuned)
  - k-nn value of "k" is tuned

* If the model is tuned using the data, the same data **cannot** be used to assess the model.

* With Cross Validation, you iteratively put data in your pocket.

* For example, keep 1/5 of the data in your pocket, build the model on the remaining 4/5 of the data.


## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide2.png")
```


## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide3.png")
```


## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide4.png")
```



## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide5.png")
```


## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide6.png")
```


## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide7.png")
```


## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide8.png")
```


## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide9.png")
```


## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide10.png")
```


## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide11.png")
```





## model parameters

* Some model parameters are tuned from the data (some aren't).
  - linear model coefficients are optimized (not tuned)
  - k-nn value of "k" is tuned

* If the model is tuned using the data, the same data **cannot** be used to assess the model.

* With Cross Validation, you iteratively put data in your pocket.

* For example, keep 1/5 of the data in your pocket, build the model on the remaining 4/5 of the data.
