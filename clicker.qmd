
# Clicker Questions

to go along with <br>

<b><a href = "https://mdsr-book.github.io/mdsr3e/" target = "_blank">Modern Data Science with R, 3rd edition</a></b> by Baumer, Kaplan, and Horton

<b><a href = "https://www.statlearning.com/" target = "_blank">Introduction to Statistical Learning with Applications in R</a></b> by James, Witten, Hastie, and Tibshirani


<!-- the two formats are html and revealjs -->


```{=html}
<style>
.reveal ol ol {
   list-style-type: lower-alpha;
}
</style>
```


```{r}
#| echo: false
#| message: false
#| warning: false

# figure options
knitr::opts_chunk$set(warning = FALSE, message = FALSE,
  fig.width = 10, fig.asp = 0.618, out.width = "90%",
  fig.retina = 3, dpi = 300, fig.align = "center"
)

library(tidyverse)

# Make sure to put a space before and after every slide break "---"

```

---

(@) The reason to take random samples is:[^1]
   (a) to make cause and effect conclusions 
   (b) to get as many variables as possible
   (c) it's easier to collect a large dataset
   (d) so that the data are a good representation of the population
   (e) I have no idea why one would take a random sample

[^1]: d. so that the data are a good representation of the population

---

(@) The reason to allocate/assign explanatory variables is:[^2]
   (a) to make cause and effect conclusions 
   (b) to get as many variables as possible
   (c) it's easier to collect a large dataset
   (d) so that the data are a good representation of the population
   (e) I have no idea what you mean by "allocate/assign" (or "explanatory variable" for that matter)

[^2]: a. to make cause and effect conclusions

---

(@) Approximately how big is a tweet?[^3]  
    (a) 0.01Kb
    (b) 0.1Kb
    (c) 1Kb
    (d) 100Kb
    (e) 1000Kb = 1Mb

[^3]: b. about 0.1Kb.  Turns out that 3.5 billion tweets * 0.1Kb = 350Gb (0.35 Tb).  My laptop is pretty good, and it has 36 Gb of memory (RAM) and 4 Tb of storage.  It would not be able to work with 3.5 billion tweets.

---

(@) $R^2$ measures:[^4]
   (a) the proportion of variability in vote margin as explained by tweet share.
   (b) the proportion of variability in tweet share as explained by vote margin.
   (c) how appropriate the linear part of the linear model is.
   (d) whether or not particular variables should be included in the model.

[^4]: a. the proportion of variability in vote margin as explained by tweet share.

---

(@) R / R Studio / Quarto[^5]
    (a) all good
    (b) started, progress is slow and steady
    (c) started, very stuck
    (d) haven’t started yet
    (e) what do you mean by "R"?

[^5]: wherever you are, make sure you are communicating with me when you have questions!

---

(@) Git / GitHub[^6]
    (a) all good
    (b) started, progress is slow and steady
    (c) started, very stuck
    (d) haven’t started yet
    (e) what do you mean by "Git"?

[^6]: wherever you are, make sure you are communicating with me when you have questions!

---

(@) Which of the following includes talking to the remote version of GitHub?[^7]
    (a) changing your name (updating the YAML)
    (b) committing the file(s)
    (c) pushing the file(s)
    (d) some of the above
    (e) all of the above
    
[^7]: d. pushing the file(s)

---

(@) What is the error?[^8]
    (a) poor assignment operator
    (b) unmatched quotes
    (c) improper syntax for function argument
    (d) invalid object name
    (e) no mistake

```{r}
#| eval: false
#| echo: true
shup2 <-- "Hello to you!"
```

[^8]: a. poor assignment operator

---

(@) What is the error?[^9]
    (a) poor assignment operator
    (b) unmatched quotes
    (c) improper syntax for function argument
    (d) invalid object name
    (e) no mistake

```{r}
#| eval: false
#| echo: true
3shup <-  "Hello to you!"
```

[^9]: d. invalid object name

---

(@) What is the error?[^10]
    (a) poor assignment operator
    (b) unmatched quotes
    (c) improper syntax for function argument
    (d) invalid object name
    (e) no mistake

```{r}
#| eval: false
#| echo: true
shup4 <-  "Hello to you!
```

[^10]: b. unmatched quotes

---

(@) What is the error?[^11]
    (a) poor assignment operator
    (b) unmatched quotes
    (c) improper syntax for function argument
    (d) invalid object name
    (e) no mistake

```{r}
#| eval: false
#| echo: true
shup5 <-  date()
```

[^11]: e. no mistake

---

(@) What is the error?[^12]
    (a) poor assignment operator
    (b) unmatched quotes
    (c) improper syntax for function argument
    (d) invalid object name
    (e) no mistake

```{r}
#| eval: false
#| echo: true
shup6 <-  sqrt 10
```

[^12]: c. improper syntax for a function argument

---

(@) Do you keep a calendar / schedule / planner?[^13]
     (a) Yes
     (b) No
     
[^13]: a. I mean, the right answer has to be Yes, right!??!

---

(@) Do you keep a calendar / schedule / planner?  If you answered "Yes" ...[^14]
     (a)	Yes, on Google Calendar
     (b)	Yes, on Calendar for macOS
     (c)	Yes, on Outlook for Windows
     (d)	Yes, in some other app
     (e)	Yes, by hand


[^14]: no right answer here!

---

(@) Where should I put things I've created for the HW (e.g., data, .ics file, etc.)[^15]
    (a) Upload into remote GitHub directory
    (b) In the local folder which also has the R project
    (c) In my Downloads
    (d) Somewhere on my Desktop
    (e) In my Home directory
    
[^15]: b. In the local folder which also has the R project.  It could be on the Desktop or the Home directory, but it must be in the same place as the R project. Do **not** upload files to the remote GitHub directory or you will find yourself with two different copies of the files.

---

(@) The goal of making a figure is...[^16]
     (a)  To draw attention to your work.
     (b) To facilitate comparisons.
     (c) To provide as much information as possible.

[^16]: Yes! All the responses are reasons to make a figure.

---

(@) A good reason to make a particular choice of a graph is:[^17]
     (a) Because the journal / field has particular expectations for how the data are presented.
     (b) Because some variables naturally fit better on some graphs (e.g., numbers on scatter plots).
     (c) Because that graphic displays the message you want as optimally as possible.

[^17]: c. Because that graphic displays the message you want as optimally as possible.

---

(@) Why are the points orange?[^18]
     (a) R translates "navy" into orange.
     (b) color must be specified in `geom_point()`
     (c) color must be specified **outside** the `aes()` function
     (d) the default plot color is orange
     
:::: {.columns}

::: {.column width="35%"}
```{r}
#| echo: false
library(mosaic)
data(Births78)
ggplot(data = Births78, 
       aes(x = date, y = births, color = "navy")) + 
  geom_point() +          
  labs(title = "US Births in 1978")
```
:::

::: {.column width="65%"}
```{r}
#| eval: false
#| echo: true
ggplot(data = Births78, 
       aes(x = date, y = births, color = "navy")) + 
  geom_point() +          
  labs(title = "US Births in 1978")
```
:::

::::

[^18]: c. color must be specified **outside** the `aes()` function

---

(@) Why are the dots blue and the lines colored?[^19]
     (a) dot color is given as "navy", line color is given as `wday`.
     (b) both colors are specified in the `ggplot()` function.
     (c) dot coloring takes precedence over line coloring.
     (d) line coloring takes precedence over dot coloring.
     
```{r}
#| echo: false
#| out-width: 60%
library(mosaic)
data(Births78)
ggplot(data = Births78, 
       aes(x = date, y = births)) + 
  geom_line(aes(color = wday)) +       
  geom_point(color = "navy") +          
  labs(title = "US Births in 1978")
```
     
[^19]: a. dot color is specified as "navy", line color is specified as `wday`.

---

(@) Setting vs. Mapping.  If I want information to be passed to all data points (not variable):[^20]
     (a) map the information inside the `aes()` function.
     (b) set the information outside the `aes()` function
     
[^20]: b. set the information outside the `aes()` function

---

(@) The Snow figure was most successful at:[^21]
     (a) making the data stand out
     (b) facilitating comparison
     (c) putting the work in context
     (d) simplifying the story

[^21]: answers may vary. I'd say c. putting the work in context.  Others might say b. facilitating comparison or d. simplifying the story.  However, I don't think a correct answer is a. making the data stand out.

---

(@) The Challenger figure(s) was(were) least successful at:[^22]
    (a) making the data stand out
    (b) facilitating comparison
    (c) putting the work in context
    (d) simplifying the story

[^22]: a. making the data stand out

---

(@) The biggest difference between Snow and the Challenger was:[^23]
    (a)	The **amount** of information portrayed.
    (b)	One was better at displaying **cause**.
    (c)	One showed the relevant **comparison** better.
    (d)	One was more **artistic**.

[^23]: c. One showed the relevant **comparison** better.

---
 
(@) Caffeine and Calories.  What was the biggest concern over the average value axes?[^24]
     (a) It isn’t at the origin.
     (b) They should have used all the data possible to find averages.
     (c) There wasn’t a random sample.
     (d) There wasn’t a label explaining why the axes were where they were.
     
[^24]: a. It isn’t at the origin. in combination with d. There wasn’t a label explaining why the axes were where they were.  The story associated with the average value axes is not clear to the reader.

---

(@) What is wrong with the following code?[^25]
    (a) should only be one `=`
    (b) `Bakery` should be upper case
    (c) `type` should not be in quotes
    (d) use mutate instead of filter
    (e) `starbucks` in wrong place

```{r}
#| eval: false
#| echo: true
Result <- |> filter(starbucks,
		type == "bakery")
```

[^25]: e. `starbucks` in wrong place

---

(@) Which data represents the ideal format for **ggplot2** and **dplyr**?[^26]

```{r}
#| echo: false
tribble_a <- tribble(
  ~year, ~Algeria, ~Brazil, ~Columbia,
  2000, 7, 12, 16,
  2001, 9, 14, 18
)
tribble_a |> gt::gt(caption = "table a") 
```

```{r}
#| echo: false
tribble_b <- tribble(
  ~country, ~Y2000, ~Y2001,
  "Algeria", 7, 9,
  "Brazil", 12, 14,
  "Columbia", 16, 18
)
tribble_b |> gt::gt(caption = "table b") 
```

```{r}
#| echo: false
tribble_c <- tribble(
  ~country, ~year, ~value,
  "Algeria", 2000, 7,
  "Algeria", 2001, 9,
  "Brazil", 2000, 12,
  "Brazil", 2001, 14,
  "Columbia", 2000, 16, 
  "Columbia", 2001, 18
)
tribble_c |> gt::gt(caption = "table c") 
```

[^26]: c. Table c is best because the columns allow us to work with each of the variable separately.

---

(@) Each of the statements except one will accomplish the same calculation.  Which one does not match?[^27]

```{r}
#| eval: false
#| echo: true
#(a) 
starbucks |> 
  group_by(type) |> 
  summarize(average_fat = mean(fat))

#(b) 
group_by(starbucks, type) |> 
  summarize(average_fat = mean(fat))

#(c)
group_by(starbucks, type) |> 
  summarize(average_fat = sum(fat))

#(d)
temp <- group_by(starbucks, type)

summarize(temp, average_fat = mean(fat))

#(e)
summarize(group_by(starbucks, type), 
          average_fat = mean(fat))
```

[^27]: c. does something different because it takes the `sum()` instead of the `mean()`.  The other commands compute the average fat broken down by `type` of Starbucks item

---

(@) Fill in Q1.[^28]
    (a) `filter()`
    (b) `arrange()`
    (c) `select()`
    (d) `mutate()`
    (e) `group_by()`
    
```{r}
#| eval: false
#| echo: true
#| code-line-numbers: "2,3"
result <- lego_sample |>
  Q1(!is.na(minifigures)) |> 
  # keep only those with minifigures
  group_by(Q2, Q2) |> 
  summarize(total = Q3)
```

[^28]: a. `filter()`

---

(@) Fill in Q2.[^29]
    (a) `(theme, price)`
    (b) `(theme, year)`
    (c) `(year, price)`
    (d) `(pieces, year)`
    (e) `(pieces, price)`
    
```{r}
#| eval: false
#| echo: true
#| code-line-numbers: "3,4"
result <- lego_sample |>
  Q1(!is.na(minifigures)) |> 
  group_by(Q2, Q2) |> 
  # for each theme and year
  summarize(total = Q3)
```

[^29]: b. `(theme, year)`

---

(@) Fill in Q3.[^30]
    (a) `n_distinct(pieces)`
    (b) `n_distinct(price)`
    (c) `sum(pieces)`
    (d) `sum(pages)`
    (e) `mean(pieces)`

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: "4,5"
result <- lego_sample |>
  Q1(!is.na(minifigures)) |> 
  group_by(Q2, Q2) |> 
  summarize(ave_pieces = Q3)
  # average number of pieces (each theme, each year)
```

[^30]: e. `mean(pieces)`

---

(@) Running the code.[^31]

```{r}
#| echo: true
library(openintro)
lego_sample |>
  filter(!is.na(minifigures)) |> 
  # keep only those with minifigures
  group_by(theme, year) |> 
  # for each theme for each year
  summarize(ave_pieces = mean(pieces))
```


```{r}
#| error: true
#| echo: true

#(a)
starbucks |> 
  group_by(type) |> 
  summarize(average_fat = mean(fat))

#(b) 
group_by(starbucks, type) |> 
  summarize(average_fat = mean(fat))

#(c)
group_by(starbucks, type) |> 
  summarize(average_fat = sum(fat))

#(d)
temp <- group_by(starbucks, type)

summarize(temp, average_fat = mean(fat))

#(e)
summarize(group_by(starbucks, type), 
          average_fat = mean(fat))
```

[^31]: running the different code chunks with relevant output.


---

---

(@) Fill in Q1.[^32]
     (a) `gdp`
     (b) `year`
     (c) `gdpval`
     (d) `country`
     (e) `–country`

```{r}
#| echo: false
library(googlesheets4)
gs4_deauth()
GDP <- read_sheet("https://docs.google.com/spreadsheets/d/1RctTQmKB0hzbm1E8rGcufYdMshRdhmYdeL29nXqmvsc/pub?gid=0")
```

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: "3"
GDP |>  
  select(country = starts_with("Income"), everything()) |> 
       pivot_longer(cols = Q1, 
                    names_to = Q2, 
                    values_to = Q3)
```

[^32]: e. `-country`

---

(@) Fill in Q2.[^33]
     (a) `gdp`
     (b) `year`
     (c) `gdpval`
     (d) `country`
     (e) `–country`

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: "4"
GDP |>  
  select(country = starts_with("Income"), everything()) |> 
       pivot_longer(cols = Q1, 
                    names_to = Q2, 
                    values_to = Q3)
```

[^33]: b. `year`

---

(@) Fill in Q3.[^34]
     (a) `gdp`
     (b) `year`
     (c) `gdpval`
     (d) `country`
     (e) `–country`

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: "5"
GDP |>  
  select(country = starts_with("Income"), everything()) |> 
       pivot_longer(cols = Q1, 
                    names_to = Q2, 
                    values_to = Q3)
```

[^34]: c. `gdpval`  (if possible, good idea to name variables something different from the name of the data frame)

---

(@) Response to stimulus (in ms) after only 3 hrs of sleep for 9 days. You want to make a plot with the subject's reaction time (y-axis) vs the number of days of sleep restriction (x-axis) using the following `ggplot()` code. Which data frame should you use?[^35]
    a. use raw data
    b. use `pivot_wider()` on raw data
    c. use `pivot_longer()` on raw data
    
```{r}
#| echo: true
#| eval: false
ggplot(___, aes(x = ___, y = ___, color = ___)) + 
  geom_line()
```

```{r}
#| echo: false
sleep_wide <- readr::read_csv("https://mac-stat.github.io/data/sleep_wide.csv")
sleep_wide
```

[^35]: c. use `pivot_longer()` on raw data.  The reference to the study is: Gregory Belenky, Nancy J. Wesensten, David R. Thorne, Maria L. Thomas, Helen C. Sing, Daniel P. Redmond, Michael B. Russo and Thomas J. Balkin (2003) Patterns of performance degradation and restoration during sleep restriction and subsequent recovery: a sleep dose-response study. Journal of Sleep Research 12, 1–12.

---

```{r}
#| echo: true
sleep_long <- sleep_wide |>
  pivot_longer(cols = -Subject,
               names_to = "day",
               names_prefix = "day_",
               values_to = "reaction_time")

sleep_long
```

---

(@) Consider band members from the Beatles and the Rolling Stones.  Who is removed in a `right_join()`?[^36]
   a. Mick
   b. John
   c. Paul
   d. Keith
   e. Impossible to know

```{r}
#| echo: true
#| eval: false
band_members |> 
  right_join(band_instruments, by = "name")
```

:::: {.columns}
::: {.column width="50%"}
```{r}
#| echo: true
band_members
```
:::

:::{.column width="50%"}
```{r}
#| echo: true
band_instruments
```
:::
::::

[^36]: a. Mick

---

(@) Consider band members from the Beatles and the Rolling Stones.  Which variables are removed in a `right_join()`?[^37]
   a. `name`
   b. `band`
   c. `plays`
   d. none of them

```{r}
#| echo: true
#| eval: false
band_members |> 
  right_join(band_instruments, by = "name")
```

:::: {.columns}
::: {.column width="50%"}
```{r}
#| echo: true
band_members
```
:::

:::{.column width="50%"}
```{r}
#| echo: true
band_instruments
```
:::
::::

[^37]: d. none of them (the default is to retain all the variables)

---

(@) What happens to Mick's `plays` variable in a `full_join()`?[^38]
   a. Mick is removed
   b. changes to guitar
   c. changes to bass
   d. `NA`
   e. `NULL`

```{r}
#| echo: true
#| eval: false
band_members |> 
  full_join(band_instruments, by = "name")
```

:::: {.columns}
::: {.column width="50%"}
```{r}
#| echo: true
band_members
```
:::

:::{.column width="50%"}
```{r}
#| echo: true
band_instruments
```
:::
::::

[^38]: d. `NA` (it would be `NULL` in **SQL**)
---

(@) Consider the `addTen()` function.  The following output is a result of which `map_*()` call?[^39]
   a. `map(c(1,4,7), addTen)`
   b. `map_dbl(c(1,4,7), addTen)`
   c. `map_chr(c(1,4,7), addTen)`
   d. `map_lgl(c(1,4,7), addTen)`
   
```{r}
#| echo: true
addTen <- function(wow) {
  return(wow + 10)
}
```

```{r}
#| echo: false
map_chr(c(1,4,7), addTen)
```

[^39]: c. `map_chr(c(1,4,7), addTen)` because the output is in quotes, the values are strings, not numbers.

---

(@) Which of the following input is allowed?[^40]
    a. `map(c(1, 4, 7), addTen)`
    b. `map(list(1, 4, 7), addTen)`
    c. `map(data.frame(a=1, b=4, c=7), addTen)`
    d. some of the above
    e. all of the above

[^40]: e. all of the above.  The `map()` function allows vectors, lists, and data frames as input.

---

(@) Which of the following produces a different output?[^41]
    a. `map(c(1, 4, 7), addTen)`
    b. `map(c(1, 4, 7), ~addTen(.x))`
    c. `map(c(1, 4, 7), ~addTen)`
    d. `map(c(1, 4, 7), function(hi) (hi + 10))`
    e. `map(c(1, 4, 7), ~(.x + 10))`
    
[^41]: c. `map(c(1, 4, 7), ~addTen)`.  The `~` acts on functions that do not have their own name or that are defined by `function(...)`.  By adding the argument `(.x)` we've expanded the `addTen()` function, and so it needs a `~`.  The `addTen()` function all alone does not use a `~`.

---

(@) What will the following code output?[^42]
    a. 3 random normals
    b. 6 random normals
    c. 18 random normals
    
```{r}
#| echo: false
#| eval: true
input <- tibble::tribble(
  ~ n, ~ mean, ~ sd,
   1,     1,    3,
   2,    3,   1,
   3,   47,  10
)
```
```{r}
#| echo: true
#| eval: true
input
```

```{r}
#| echo: true
#| eval: false
input |> 
  pmap(rnorm)
```
[^42]: b. 6 random normals (1 with mean 1, sd 3; 2 with mean 3, sd 1; 3 with mean 47, sd 10)

---

(@) In R the `ifelse()` function takes the arguments:[^43]
   a. question, yes, no
   b. question, no, yes
   c. statement, yes, no
   d. statement, no, yes
   e. option1, option2, option3


[^43]: a. question, yes, no

---

(@) What is the output of the following:[^44]
    a.	"cat", 30, "cat", "cat", 6 
    b.	"cat", "30", "cat", "cat", "6"
    c.	1, "cat", 5, "cat", "cat"
    d.	1, "cat", 5, NA, "cat"
    e.	"1", "cat", "5", NA, "cat"

```{r}
#| eval: false
#| echo: true
data <- c(1, 30, 5, NA, 6)

ifelse(data > 5, "cat", data)
```

[^44]: e.	"1", "cat", "5", NA, "cat"  (Note that the numbers were converted to character strings!)

---

(@) In R, the `set.seed()` function[^45]
   a. makes your computations go faster
   b. keeps track of your computation time
   c. provides an important parameter
   d. repeats the function
   e. makes your results reproducible

[^45]: e. makes your results reproducible


---

(@) If I run a hypothesis test with a type I error cut off of $\alpha = 0.05$ and the null hypothesis is true, what is the probability of rejecting $H_0$?[^46]
   a. 0.01
   b. 0.05
   c. 0.1
   d. I don't know.
   e. No one knows.


[^46]: b. 0.05  If the null hypothesis is true and the technical conditions hold, then we should reject the null hypothesis $\alpha \cdot 100$% of the time.

---

(@) If I run a hypothesis test with a type I error cut off of $\alpha = 0.05$ and the null hypothesis is true, **and also the technical conditions do not hold** what is the probability of rejecting $H_0$?[^47]
   a. 0.01
   b. 0.05
   c. 0.1
   d. I don't know.
   e. No one knows.


[^47]: e. No one knows.  It totally depends on how and how much the technical conditions are violated and how resistant the test is to the technical conditions.

---

(@) If I run a hypothesis test with a type I error cut off of $\alpha = 0.05$ and **the null hypothesis is false**, what is the probability of rejecting $H_0$?[^48]
   a. 0.01
   b. 0.05
   c. 0.1
   d. I don't know.
   e. No one knows.


[^48]: e. No one knows.  It totally depends on the degree to which the null hypothesis is false.

---

(@) Which is the best conclusion to the following?  Changes in the technical conditions change the...[^48a]
     a. choice of test statistic
     b. distribution of the test statistic (the sampling distribution)
     c. computation of the p-value using standard software
     d. hypotheses

[^48a]: b. distribution of the test statistic (the sampling distribution)

---

(@) If I aim to create a 95% confidence interval, and the technical conditions hold, what is the probability that the CI will contain the true value of the parameter?[^49]
   a. 0.90
   b. 0.95
   c. 0.99
   d. I don't know.
   e. No one knows.


[^49]: b. 0.95  If the technical conditions hold, 95% of all confidence intervals should contain the true parameter.

---

(@) If I aim to create a 95% confidence interval, and **the technical conditions do not hold**, what is the probability that the CI will contain the true value of the parameter?[^50]
   a. 0.90
   b. 0.95
   c. 0.99
   d. I don't know.
   e. No one knows.


[^50]: e. No one knows.  If the technical conditions do not hold, the CI may or may not contain the true value of the parameter at the given confidence level (i.e., 95%).

---

(@) We typically compare means (across two groups) instead of medians because:[^51]
   a. we don’t know the SE of the difference of medians
   b. means are inherently more interesting than medians
   c. permutation tests don’t work with medians
   d. the Central Limit Theorem doesn’t apply for medians.
   
[^51]: d. the Central Limit Theorem doesn’t apply for medians.

---

(@) What are the technical assumptions for a t-test?[^52]
   a. none
   b. normal data
   c. $n \geq 30$
   d. random sampling / random allocation for appropriate conclusions

[^52]: we always need d. random sampling / random allocation for appropriate conclusions.  The theory is derived from b. normal data.  If c. $n \geq 30$, then the theory holds really well, regardless of whether the data are normal.

---

(@) What are the technical conditions for permutation tests?[^53]
   a. none
   b. normal data
   c. $n \geq 30$
   d. random sampling / random allocation for appropriate conclusions
   
[^53]: d. random sampling / random allocation for appropriate conclusions
   
---

(@) Follow up to permutation test:  the assumptions change based on whether the statistic used is the mean, median, proportion, etc.[^54]
   a. TRUE
   b. FALSE

[^54]: b. FALSE


---

(@) Why care about the distribution of the test statistic?[^55]
   a. Better estimator
   b. So we can find rejection region
   c. So we can control power
   d. Because we love the CLT
   
[^55]: b. So we can find rejection region

---

(@) Given statistic T = r(X), how do we find a (sensible) test?[^56]
   a. Maximize power
   b. Minimize type I error
   c. Control type I error
   d. Minimize type II error
   e. Control type II error

[^56]: c. Control type I error

---

The group averages for the next few questions:

```{r}
#| echo: true
library(NHANES)
GM <- NHANES  |> summarize(mean(HHIncomeMid, na.rm=TRUE))  |> pull()

NH.means <- NHANES  |> 
  filter(!is.na(HealthGen) & !is.na(HHIncomeMid))  |> 
  group_by(HealthGen)  |> 
  summarize(IncMean = mean(HHIncomeMid), count=n())
NH.means
```


---

(@) The following code calculates which part of the test statistic?[^54a]
     a. $\overline{X}$
     b. $(\overline{X}_{i\cdot} - \overline{X})$
     c. $(\overline{X}_{i\cdot} - \overline{X})^2$
     d. $n_i$
     e. $n_i \cdot (\overline{X}_{i\cdot} - \overline{X})^2$

```{r}
#| eval: true
#| echo: true
NH.means  |> select(IncMean)  |> pull() - GM
```


[^54a]: b. $(\overline{X}_{i\cdot} - \overline{X})$

---

(@) The following code calculates which part of the test statistic?[^54b]
     a. $\overline{X}$
     b. $(\overline{X}_{i\cdot} - \overline{X})$
     c. $(\overline{X}_{i\cdot} - \overline{X})^2$
     d. $n_i$
     e. $n_i \cdot (\overline{X}_{i\cdot} - \overline{X})^2$
     
```{r}     
#| eval: true
#| echo: true
(NH.means  |> select(IncMean)  |> pull() - GM)^2
```

[^54b]: c. $(\overline{X}_{i\cdot} - \overline{X})^2$

---

(@) The following code calculates which part of the test statistic?[^54c]
     a. $\overline{X}$
     b. $(\overline{X}_{i\cdot} - \overline{X})$
     c. $(\overline{X}_{i\cdot} - \overline{X})^2$
     d. $n_i$
     e. $n_i \cdot (\overline{X}_{i\cdot} - \overline{X})^2$
     
```{r}
#| eval: true
#| echo: true
NH.means  |> select(count)  |> pull()
```

[^54c]: d. $n_i$

---

(@) The following code calculates which part of the test statistic?[^54d]
     a. $\overline{X}$
     b. $(\overline{X}_{i\cdot} - \overline{X})$
     c. $(\overline{X}_{i\cdot} - \overline{X})^2$
     d. $n_i$
     e. $n_i \cdot (\overline{X}_{i\cdot} - \overline{X})^2$

```{r}
#| eval: true
#| echo: true
NH.means  |> select(count)  |> pull() * 
  (NH.means  |> select(IncMean)  |> pull() - GM)^2
```

[^54d]: e. $n_i \cdot (\overline{X}_{i\cdot} - \overline{X})^2$


---

(@) Type I error is[^57]
   a. We give him a raise when he deserves it.
   b. We don’t give him a raise when he deserves it.
   c. We give him a raise when he doesn’t deserve it.
   d. We don’t give him a raise when he doesn’t deserve it.

[^57]: c. We give him a raise when he doesn’t deserve it.

---

(@) Type II error is[^58]
   a. We give him a raise when he deserves it.
   b. We don’t give him a raise when he deserves it.
   c. We give him a raise when he doesn’t deserve it.
   d. We don’t give him a raise when he doesn’t deserve it.

[^58]: b. We don’t give him a raise when he deserves it.

---

(@) Power is the probability that:[^59]
   a. We give him a raise when he deserves it.
   b. We don’t give him a raise when he deserves it.
   c. We give him a raise when he doesn’t deserve it.
   d. We don’t give him a raise when he doesn’t deserve it.

[^59]: a. We give him a raise when he deserves it.

---

(@) Why don’t we always reject $H_0$?[^60]
    a. type I error too high
    b. type II error too high
    c. level of sig too high
    d. power too high

[^60]: a. type I error too high

---

(@) The player is more worried about[^61]
    a. A type I error
    b. A type II error

[^61]: b. A type II error

---

(@) The coach is more worried about[^62]
    a. A type I error
    b. A type II error

[^62]: a. A type I error

---

(@) Increasing your sample size[^63]
    a. Increases your power
    b. Decreases your power
   
[^63]: a. Increases your power

---

(@) Making your significance level more stringent ($\alpha$ smaller)[^64]
   a. Increases your power
   b. Decreases your power

[^64]: b. Decreases your power

---

(@) A more extreme alternative[^65]
    a. Increases your power
    b. Decreases your power
   
[^65]: a. Increases your power

---

(@) For the MacNell study, how should the data be permuted to address the question about *perceived* gender?[^65a]
    a. Permute the identity variable (perceived gender)
    b. Permute the gender variable (actual gender)
    c. Permute the gender variable after grouping by the identity variable
    d. Permute the identity variable after grouping by the gender variable

[^65a]: d. Permute the identity variable after grouping by the gender variable

---

(@) In order to "Permute the identity variable after grouping by the gender variable" we should `group_by()`:[^65b]
    a. the identity variable (perceived gender)
    b. the gender variable (actual gender)
    c. both the gender variable and the identity variable
    d. neither the identity variable nor the gender variable

[^65b]: b. the gender variable (actual gender)

---

(@) In order to create a null sampling distribution, we need to calculate the statistic (difference in average score), which requires a `group_by()` of:[^65c]
    a. the identity variable (perceived gender)
    b. the gender variable (actual gender)
    c. both the gender variable and the permuted identity variable
    d. the permuted identity variable
    e. we don't need to use `group_by()`

[^65c]: d. the permuted identity variable

---

(@) What is the primary reason to use a permutation test (instead of a test built on calculus)?[^66]
   a. more power
   b. lower type I error
   c. more resistant to outliers
   d. can be done on statistics with unknown sampling distributions

[^66]: d. can be done on statistics with unknown sampling distributions

---

(@) What is the primary reason to bootstrap a CI (instead of creating a CI from calculus)?[^67]
   a. larger coverage probabilities
   b. narrower intervals
   c. more resistant to outliers
   d. can be done on statistics with unknown sampling distributions

[^67]: d. can be done on statistics with unknown sampling distributions

---

(@) Which of the following could not possibly be a bootstrap sample from the vector: `c(4, 10, 8, 1, 2, 4)`[^67b]
    a. `c(4, 4, 4, 4, 4, 4)`
    b. `c(4, 10, 8, 1, 2, 4)`
    c. `c(1, 2, 2, 4, 4, 2)`
    d. `c(10, 8, 1, 1, 8, 10)`
    e. `c(1, 2, 4, 3, 4, 10)`

[^67b]: e. `c(1, 2, 4, 3, 4, 10)` because there is no 3 in the original dataset.

---

(@) You have a sample of size n = 50.  You sample with replacement 1000 times to get 1000 bootstrap samples. What is the sample size of each bootstrap sample?[^68]
   a. 50
   b. 1000

[^68]: a. 50

---

(@) You have a sample of size n = 50.  You sample with replacement 1000 times to get 1000 bootstrap samples. How many bootstrap statistics will you have?[^69]
   a. 50
   b. 1000

[^69]: b. 1000

---

(@) The bootstrap distribution is centered around the[^70]
   a. population parameter
   b. sample statistic
   c. bootstrap statistic
   d. bootstrap parameter

[^70]: b. sample statistic

---

(@) In $B$ different random samples, how many values for $\overline{X}$ do you have? How many values for $SE(\overline{X}) = s / \sqrt{n}$ do you have?[^70a]
    a. 1, 1
    b. 1, B
    c. B, 1
    d. B, B

[^70a]: d. B, B

---

(@) In $B$ bootstrap re-samples, how many values for $\hat{\theta}^*(b)$ do you have? How many values for $\hat{SE}^*$ do you have?[^70b]
    a. 1, 1
    b. 1, B
    c. B, 1
    d. B, B

[^70b]: c. B, 1

---

(@) What is wrong with using $\frac{\hat{\theta}^*(b) - \hat{\theta}}{\hat{SE}^*}$ to estimate $\frac{\hat{\theta} - \theta}{SE(\hat{\theta})}?$[^70c]
    a. bootstrap version is too variable
    b. bootstrap version is not variable enough
    c. bootstrap version is biased
    d. bootstrap version measures the wrong thing entirely
    

[^70c]: b. bootstrap version is not variable enough


---

(@) Using our bootstrap information, we can estimate the $\alpha/2$ percentile of the distribution of $\frac{\hat{\theta}-\theta}{SE(\hat{\theta})}$ as $\hat{t}^*_{\alpha/2}$ using:[^70d]
     a. $\frac{\#  \bigg\{\frac{\hat{\theta}-\theta}{SE(\hat{\theta})} \leq \ \hat{t}^*_{\alpha/2} \bigg\} }{B} = \alpha/2$
     b. $\frac{\#  \bigg\{\frac{\hat{\theta}-\theta}{SE(\hat{\theta})} \leq \ \hat{t}^*_{\alpha/2} \bigg\} }{B} = \alpha$
     c. $\frac{\#  \bigg\{\frac{\hat{\theta}^*(b)-\hat{\theta}}{\hat{SE}^*(b)} \ \leq \hat{t}^*_{\alpha/2} \bigg\} }{B} = \alpha/2$
     d. $\frac{\#  \bigg\{\frac{\hat{\theta}^*(b)-\hat{\theta}}{\hat{SE}^*(b)} \ \leq \hat{t}^*_{\alpha/2} \bigg\} }{B} = \alpha$

[^70d]: c. $\frac{#  \{\frac{\hat{\theta}^*(b)-\hat{\theta}}{\hat{SE}^*(b)} \leq \hat{t}^*_{\alpha/2} \} }{B} = \alpha/2$

---

(@) Is the bootstrap sampling distribution of $\frac{\hat{\theta}^*(b)-\hat{\theta}}{\hat{SE}^*(b)}$ symmetric? That is, does $\hat{t}^*_{\alpha/2} = -\hat{t}^*_{1 - \alpha/2}$?[^70e]
      a. TRUE
      b. FALSE

[^70e]: b. FALSE. There is no reason to assume it would necessarily be symmetric.


---

(@) 95% CI for the difference in proportions:[^71]
    a. (0.15, 0.173)
    b. (0.025, 0.975)
    c. (0.72, 0.87)
    d. (0.70, 0.873)
    e. (0.12, 0.179)
   
```{r out.width='60%', fig.align="center",  echo=FALSE, fig.cap = "https://www.lock5stat.com/StatKey/bootstrap_2_cat/bootstrap_2_cat.html"}
knitr::include_graphics("images/BShist.jpg")
```

[^71]: e. (0.12, 0.179)

---

(@) Suppose a 95% bootstrap CI for the difference in trimmed means was (3,9), would you reject H0?[^72] (uh... What is the null hypothesis here???)
   a. yes
   b. no
   c. not enough information to know

[^72]: a. yes (because the interval for the true difference in population trimmed means does not overlap zero.)

---

(@) Given the situation where $H_a$ is TRUE.  Consider 100 CIs (for true difference in means, where each of the 100 CIs is created using a different dataset). The power of the test can be approximated by:[^73]
   a. The proportion that contain the true difference in means.
   b. The proportion that do not contain the true difference in means.
   c. The proportion that contain zero.
   d. The proportion that do not contain zero.

[^73]: d. The proportion that do not contain zero.


---

(@) Which of the following best describes feature engineering?[^73a]
  a. The process of choosing the best machine learning algorithm for a dataset
  b. The process of designing new variables or transforming existing ones to improve model performance
  c. The process of tuning hyperparameters for a model
  d. The process of splitting the data into training and testing sets

[^73a]: b. The process of designing new variables or transforming existing ones to improve model performance

---

(@) You have a categorical variable color with three levels: "red", "blue", and "green". With dummy coding (with a reference level), how many new variables are created?[^73b]
   a. 1
   b. 2
   c. 3
   d. 4
   
[^73b]: b. 2  Let's say "red" is the reference level, then there are two binary variables (blue: yes/no and green: yes/no). Dummy variables are used in models like linear and logistic regression where there is a reference value and you want to be able to interpret the coefficients. Use `step_dummy()`.

---


(@) You have a categorical variable color with three levels: "red", "blue", and "green". With one-hot encoding (without a reference level), how many new variables are created?[^73c]
   a. 1
   b. 2
   c. 3
   d. 4
   
[^73c]: b. 3  There are thre binary variables (red: yes/no; blue: yes/no; green: yes/no). One-hot encoding is used in models where there is no reference variable and you want all categories to be represented. Use `step_dummy(..., one_hot = TRUE)`.

---

(@) Which of the following best describes data leakage in feature engineering?[^73d]
   a. Using too many features in the model
   b. Including features that are highly correlated
   c. Using information from the test set when creating features or training the model
   d. Not standardizing numerical variables before training

[^73d]: c. Using information from the test set when creating features or training the model

---

(@) Small k in k-NN will help reduce the risk of overfitting.[^74]
    a. TRUE
    b. FALSE

[^74]: b. FALSE

---

(@) The training error for 1-NN classifier is zero.[^75]
    a. TRUE
    b. FALSE

[^75]: a. TRUE

---

(@) Generally, the k-NN algorithm can take any distance measure.[^76]
   a. TRUE
   b. FALSE

[^76]: a. TRUE

---

(@) In R, the `kknn` method can use any distance measure.[^77]
   a. TRUE
   b. FALSE

[^77]: b. FALSE, it uses Minkowski(p) distance, with a user specified choice of `p`.  When `p=2`, Minkowski is the same as Euclidean.

---

(@) The `k` in k-NN refers to[^78]
    a. `k` groups
    b. `k` partitions
    c. `k` neighbors

[^78]: c. `k` neighbors

---

(@) the `V` in V-fold CV refers to[^79]
    a. `V` groups
    b. `V` partitions
    c. `V` neighbors

[^79]: b. `V` partitions

---

(@) All of the following are TRUE for the use of CART except for:[^80]
   a. Can deal with missing data
   b. Require the assumptions of statistical models
   c. Variable selection is automatic
   d. Produce rules that are easy to interpret and implement

[^80]: b. Require the assumptions of statistical models

---

(@) Simplifying the decision tree by pruning peripheral branches will cause overfitting.[^81]
   a. TRUE
   b. FALSE

[^81]: b. FALSE.  If you don't prune, you will overfit.

---

(@) All are true with regards to PRUNING except:[^82]
   a. Multiple (sequential) trees are possible to create by pruning
   b. CART lets tree grow to full extent, then prunes it  
   c. Pruning generates successively smaller trees by pruning leaves
   d. Pruning is only beneficial when purity improvement is statistically significant

[^82]: d. Pruning is only beneficial when purity improvement is statistically significant (we don't do hypothesis testing on trees)

---

(@) Regression trees are invariant to monotonic transformations of:[^83]
   a. the explanatory (predictor) variables
   b. the response variable
   c. both types of variables
   d. none of the variables

[^83]: a. the explanatory (predictor) variables

---

(@) CART suffers from[^84]
    a. high variance
    b. high bias

[^84]: a. high variance

---

(@) bagging uses bootstrapping on:[^85]
    a. the variables
    b. the observations
    c. both
    d. neither

[^85]: b. the observations

---

(@) oob samples[^86]
   a. are in the test data
   b. are in the training data and provide independent predictions
   c. are in the training data but do not provide independent predictions

[^86]: b. are in the training data and provide independent predictions

---

(@) oob samples are great because[^87]
   a. oob is "boo" spelled backwards
   b. oob samples allow for independent predictions
   c. oob samples allow for more predictions than a "test group"
   d. oob data frame is always bigger than the test sample data frame
   e. some of the above

[^87]: e. some of the above (both of b. and c. are great!) The oob data frame is exactly the same size as the training data, and it may or may not be bigger than the test data.

---

(@) bagging is random forests with:[^88]
   a. m = # predictor variables
   b. all the observations
   c. the most important predictor variables isolated
   d. cross validation to choose m

[^88]: a. m = # predictor variables

---

(@) We have 80 training observations and 20 test observations. To get the test MSE, we need[^89]
   a. 20 predictions from all trees
   b. 20 predictions from oob trees
   c. 80 predictions form all trees
   d. 80 predictions from oob trees

[^89]: a. 20 predictions from all trees

---

(@) With random forests, the value for m is chosen[^90]
   a. using OOB error rate
   b. as p/3
   c. as sqrt(p)
   d. using cross validation 

[^90]: d. using cross validation  (but is also often used as p/3 or sqrt(p))

---

(@) A tuning parameter:[^91]
   a. makes the model fit the training data as well as possible.
   b. makes the model fit the test data as well as possible.
   c. allows for a good model that does not overfit the data.
   
[^91]: c. allows for a good model that does not overfit the data.

---

(@) With binary response and X1 and X2 continuous, kNN (k=1) creates a linear decision boundary.[^92]
   a. TRUE
   b. FALSE

[^92]: b. FALSE

---

(@) With binary response and X1 and X2 continuous, a classification tree with one split creates a linear decision boundary.[^93]
   a. TRUE
   b. FALSE

[^93]: a. TRUE

---

(@) With binary response and X1 and X2 continuous, a classification tree with one split creates **the best** linear decision boundary.[^94]
   a. TRUE
   b. FALSE
   
[^94]: b. FALSE  (what is "best" ????)

---

(@)  If the data are linearly separable, there exists a "widest street".[^95]
   a. yes
   b. no
   c. up to a constant
   d. with the alpha values "tuned" appropriately

[^95]: a. yes

---

(@) In the case of linearly separable data, the SVM:[^96]
   a. has a tuning parameter of $\alpha$
   b. has a tuning parameter of dimension
   c. has no tuning parameters

[^96]: c. has no tuning parameters

---

(@) Linear models are similar to SVM with linearly separable data in that they optimize the model instead of tuning it.[^97]
    a. TRUE
    b. FALSE
    
[^97]: a. TRUE

---

(@) If the data have a complex boundary, the value of gamma in RBF kernel should be:[^98]
    a. Big
    b. Small

[^98]: a. Big

---

(@) If the data have a simple boundary, the value of gamma in RBF kernel should be:[^99]
    a. Big
    b. Small

[^99]: b. Small

---

(@) If I am using all features of my dataset and I achieve 100% accuracy on my training set, but ~70% on the cross validation set, what should I look out for?[^100]
     a. Underfitting 
     b. Nothing, the model is perfect
     c. Overfitting

[^100]: c. Overfitting

---

(@) For a large value of C, the model is expected to[^101]
     a. overfit the training data more as compared to a small C
     b. overfit the training data less as compared to a small C
     c. not related to overfitting the data

[^101]: a. overfit the training data more as compared to a small C (because the act of misclassifying is heavily penalized)

---

(@) Suppose you have trained an SVM with linear decision boundary. You correctly infer that your training SVM model is underfitting. Which of the following should you consider?[^102]
     a. increase number of observations
     b. decrease number of observations
     c. calculate more variables
     d. reduce the number of features

[^102]: c. calculate more variables (use feature engineering to see if you can get more information out of the variables)

---

(@) Suppose you have trained an SVM with linear decision boundary. You correctly infer that your training SVM model is underfitting. Suppose you gave the correct answer in previous question. What do you think that is actually happening (when you take that action)?[^103]
i. We are lowering the bias
ii. We are lowering the variance
iii. We are increasing the bias
iv. We are increasing the variance
     a. i. and ii.
     b. ii. and iii.
     c. i. and iv.
     d. ii. and iv. 

[^103]: c. i. and iv.  The model is too simple (i.e., biased), so we need more information to make it more complex.  If we make the model more complex it will have lower bias but higher variance.

---

(@) Suppose you are using SVM with polynomial kernel of degree 2. Your model perfectly predicts! That is, training and testing accuracy are both 100%. You increase the complexity (degree of polynomial). What will happen?[^104]
   a. Increasing the complexity will overfit the data (increase variance)
   b. Increasing the complexity will underfit the data (increase bias)
   c. Nothing will happen since your model was already 100% accurate
   d. None of these
   
[^104]: a. Increasing the complexity will overfit the data (increase variance)  Even though you already perfectly fit the data, the model could potentially draw boundaries that were even more singular, thus increasing the variance and producing a worse model.

---

(@) Building on the previous question, after increasing the complexity, you found that training accuracy was still 100%. According to you what is the reason behind that?[^105]
     i. Since data are fixed and we are fitting more polynomial terms, the algorithm starts memorizing everything in the data
     ii. Since data are fixed, SVM doesn’t need to search in high dimensional space

     a. i.
     b. ii.
     c. i. and ii.
     d. none of these

[^105]: a. i.  Effectively.  The polynomial bound becomes more wiggly as the degree of the polynomial increases.

---

(@) The cost parameter in the SVM means:[^106]
     a. The number of cross-validations to be made
     b. The kernel to be used
     c. The trade-off between misclassification and simplicity of the model
     d. None of the above

[^106]: c. The trade-off between misclassification and simplicity of the model (In the SVM derivation, we think of it as the trade-off between the misclassifications and the width of the street.  But that width determines how complicated the model is.)

--- 

(@) Suppose you have trained an SVM classifier with a Gaussian kernel, and it learned the following decision boundary on the training set. You suspect that the SVM is underfitting your dataset. What should you try?[^107]
    a. decreasing C and/or decrease gamma
    b. decreasing C and/or increase gamma
    c. increasing C and/or decrease gamma
    d. increasing C and/or increase gamma

```{r out.width='60%', fig.align="center",  echo=FALSE}
knitr::include_graphics("images/svm_linear.png")
```

[^107]: d. increasing C (to discourage misclassifications) and/or increase gamma (to encourage more complicated models)

---

(@) Suppose you have trained an SVM classifier with a Gaussian kernel, and it learned the following decision boundary on the training set. When you measure the SVM’s performance on a cross validation set, it does poorly. What should you try?[^108]
    a. decreasing C and/or decrease gamma
    b. decreasing C and/or increase gamma
    c. increasing C and/or decrease gamma
    d. increasing C and/or increase gamma

```{r out.width='60%', fig.align="center",  echo=FALSE}
knitr::include_graphics("images/svm_nonlinear.png")
```

[^108]: a. decreasing C (to allow more misclassifications) and/or decrease gamma (to make a simpler model)

---

(@)  Cross validation will guarantee that the model does not overfit.[^109]
     a. TRUE
     b. FALSE

[^109]: b. FALSE  Nothing in statistics (or life) is guaranteed. Tuning parameters, however does help the model to avoid overfitting as much as possible.

---

(@) The biggest problem with missing data is the resulting small sample size.[^110]
     a. TRUE
     b. FALSE
     
[^110]: b. FALSE  The biggest problem with missing data is that missingness is almost always non-random. So the missing values are systematically different than the non-missing data.  Makes conclusions difficult.

--- 

(@)	Which statement is not true about cluster analysis?[^111]
    a. Objects in each cluster tend to be similar to each other and dissimilar to objects in the other clusters.
    b. Cluster analysis is a type of unsupervised learning.
    c. Groups or clusters are suggested by the data, not defined a priori.
    d. Cluster analysis is a technique for analyzing data when the response variable is categorical and the predictor variables are continuous in nature.

[^111]: d. Cluster analysis is a technique for analyzing data when the response variable is categorical and the predictor variables are continuous in nature.

---

(@)	A _____ or tree graph is a graphical device for displaying clustering results.  Vertical lines represent clusters that are joined together.  The position of the line on the scale indicates the distances at which clusters were joined. [^112]
    a. dendrogram
    b. scatterplot
    c. scree plot
    d. segment plot 

[^112]: a. dendrogram

---

(@)	_____ is a clustering procedure characterized by the development of a tree-like structure.[^113]
     a. Partitioning clustering  
     b. Hierarchical clustering
     c. Divisive clustering
     d. Agglomerative clustering

[^113]: b. Hierarchical clustering

---


(@)	_____ is a clustering procedure where all objects start out in one giant cluster. Clusters are formed by dividing this cluster into smaller and smaller clusters.[^114]
    a. Non-hierarchical clustering
    b. Hierarchical clustering
    c. Divisive clustering
    d. Agglomerative clustering

[^114]: c. Divisive clustering

---

(@) The _____ method uses information on all pairs of distances, not merely the minimum or maximum distances.[^115]
    a. single linkage
    b. medium linkage
    c. complete linkage
    d. average linkage

[^115]: d. average linkage

---

(@) Hierarchical clustering is deterministic, but k-means clustering is not.[^116]
     a. TRUE
     b. FALSE

[^116]: a. TRUE  (k-means starts with a random set of centers)

---

(@)	k-means	is a clustering procedure characterized referred to as ________.[^117]
     a. Partitioning clustering  
     b. Hierarchical clustering
     c. Divisive clustering
     d. Agglomerative clustering

[^117]: a. Partitioning clustering

---

(@) One method of assessing reliability and validity of clustering is to use different methods of clustering and compare the results.[^118]
     a. TRUE
     b. FALSE

[^118]: a. TRUE

---

(@) The choice of k, the number of clusters to partition a set of data into,...[^119]
    a. is a personal choice that shouldn't be discussed in public
    b. depends on why you are clustering the data
    c. should always be as large as your computer system can handle
    d. has maximum 10

[^119]: b. depends on why you are clustering the data

---

(@) Which of the following is required by k-means clustering?[^120]
    a. defined distance metric
    b. number of clusters
    c. initial guess as to cluster centroids
    d. all of the above
    e. some of the above

[^120]: d. all of the above

---

(@) For which of the following tasks might clustering be a suitable approach?[^121]
     a. Given sales data from many products in a supermarket, estimate future sales for each of these products.
     b. Given a database of information about your users, automatically group them into different market segments.
     c. From the user's usage patterns on a website, identify different user groups.
     d. Given historical weather records, predict if tomorrow's weather will be sunny or rainy.

[^121]: c. From the user's usage patterns on a website, identify different user groups.  Or maybe b.  No pre-defined response variable.

---

(@) k-means is an iterative algorithm, and two of the following steps are repeatedly carried out. Which two?[^122]
1. Assign each point to its nearest cluster
2. Test on the cross-validation set
3. Update the cluster centroids based the current assignment
4. Using the elbow method to choose K
     a. 1 & 2
     b. 1 & 3
     c. 1 & 4
     d. 2 & 3
     e. 2 & 4
     f. 3 & 4

[^122]: b. 1 & 3

---




