---
title: "07. Recipes + k-NN"
description: |
  And old adage says: garbage in, garbage out.  Here we avoid garbage in.
  k-Nearest Neighbors is a classification algorithm based on the premise that points which are close to one another (in some predictor space) are likely to be similar with respect to the outcome variable.
author:
  - name: Johanna Hardin
    url: https://m154-comp-stats.netlify.app/  
date: 2025-10-20
citation: false
embed-resources: true
execute: 
  message: false
  warning: false
image: ../../images/recipes.png
---

```{r fig.cap = "Artwork by @allison_horst.", fig.alt = "", preview = TRUE, echo = FALSE}
knitr::include_graphics("../../images/recipes.png")
```




## Agenda <i class="fas fa-calendar-alt" target="_blank"></i>

### October 20, 2025

1. What needs to be done to the data?
2. `tidymodels` syntax for recipes
3. Example


### October 22, 2025

1. $k$-Nearest Neighbors
2. Cross Validation
3. Example


## Readings <i class="fas fa-book-open"></i> 


* Class notes: <a href = "https://st47s.com/Math154/Notes/08-classification.html#model-building-process" target= "_blank">model building</a>

* Class notes: <a href = "https://st47s.com/Math154/Notes/08-classification.html#knn" target= "_blank">k nearest neighbors</a>

* Max Kuhn and Julia Silge (2021), <a href = "https://www.tmwr.org/" target = "_blank">Tidy Modeling with R</a>

## Reflection questions <i class="fas fa-lightbulb"></i>

* What is the process for building a model using **tidymodels**?

* Why is it important to do feature engineering for variables in a model?

* How is data separated in order to work with independent information (hint: two ways)?

* What is the "$k$" in $k$-Nearest Neighbors?  

* Why do most implementations of $k$-NN prefer odd values of k?

* How does $k$-NN make predictions on test data?

* Can $k$-NN  be used for both classification and regression or only one of the two tasks?

* Can you use categorical / character predictors with $k$-NN?

* How is $k$ chosen?

* How do the bias and variance change for different values of $k$ in $k$-NN?

* What are the advantages of the $k$-NN algorithm?

* What are the disadvantages of the $k$-NN algorithm?


## Ethics considerations <i class="fas fa-balance-scale"></i> 

* There are two ways that laws are enforced (both equally important):

   1. disparate treatment $\rightarrow$ means that the differential treatment is intentional

   2. disparate impact $\rightarrow$ means that the differential treatment is unintentional or implicit (some examples include advancing mortgage credit, employment selection, predictive policing)

* Anti-discrimination Laws 

    * Civil Rights Acts of 1964 and 1991
    * Americans with Disabilities Act
    * Genetic Information Nondiscrimination Act
    * Equal Credit Opportunity Act
    * Fair Housing Act 


* Questions to ask yourself in every single data analysis you perform (taken from <a href = "https://dssg.uchicago.edu/" target = "_blank">Data Science for Social Good</a> at the University of Chicago):

  * What biases may exist in the data you've been given?  How can you find out?
  * How will your choices with tuning parameters affect different populations represented in the data?
  * How do you know you aren't getting the right answer to the    wrong question?
  * How would you justify what you'd built to someone whose welfare is made worse off by the implementation of your algorithm?
  * See the slides on bias in modeling (9/23/21) for times when there are no inherent biases but the structure of the data create unequal model results.
  
* What type of feature engineering is required for $k$-NN?

* Why is it recommended that $k$-NN not be used on large datasets?

* (For a given $k$) why does $k$-NN use more computation time to test than to train?  [n.b., the opposite is true for the majority of classification and regression algorithms.]

* If the model produces near perfect predictions on the test data, what are some potential concerns about putting that model into production?


## Slides <i class="fas fa-desktop"></i> 


* <a href = "https://m154-comp-stats.netlify.app/slides/2025-10-20-recipes" target = "_blank">Recipes + feature engineering</a> for 10/20/2025.

* <a href = "https://m154-comp-stats.netlify.app/slides/2024-10-23-knn" target = "_blank">k-NN</a> for 10/22/2025.

* <a href = "https://m154-comp-stats.netlify.app/handout/ws12_m154_f25_FE.pdf" target = "_blank">WS12 - feature engineering</a>

## Additional Resources <i class="fas fa-laptop"></i> 

* Hilary Mason <a href = "https://www.youtube.com/watch?v=5q87K1WaoFI" target = "_blank">describing</a> what is machine learning to 5 different people.


* <a href = "https://juliasilge.com/" target = "_blank">Julia Silge's blog</a> is full of complete **tidymodels** examples and screencasts.

* Alexandria Ocasio-Cortez, Jan 22, 2019  <a href = "http://aaronsadventures.blogspot.com/2019/01/discussion-of-unfairness-in-machine.html" target = "_blank">MLK event with Ta-Nehisi Coates</a>

* S. Barocas and A. Selbst, "Big Data's Disparate Impact", *California Law Review, 671*, 2016.

* <a href = "https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing/" target = "_blank">Machine Bias</a> in *Pro Publica* by Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner, May 23, 2016 

* <a href = "https://www.ajlunited.org/" target = "_blank">Algorithmic Justice League</a> is a collective that aims to:

   * Highlight algorithmic bias through media, art, and science

   * Provide space for people to voice concerns and experiences with coded bias

   * Develop practices for accountability during design, development, and deployment of coded systems

   * Joy Buolamwini -- <a href = "https://www.youtube.com/embed/QxuyfWoVV98" target = "_blank">AI, Ain't I A Woman?</a>




