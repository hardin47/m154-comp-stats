---
title: "11. Unsupervised Learning"
description: |
  A quick dive into unsupervised methods.  We cover two clutering methods: hierarchical and partitioning (k-means and k-medoids). Additionally, we discuss latent Dirichlet allocation (LDA).
author:
  - name: Johanna Hardin
    url: https://m154-comp-stats.netlify.app/  
date: 2025-11-19
citation: false
embed-resources: true
execute: 
  message: false
  warning: false
image: ../../images/kmeans_5.jpg
---


```{r fig.cap = "Artwork by @allison_horst.", fig.alt = "Monsters as cluster centers moving around throughout the k-means algorithm.", preview = TRUE, echo = FALSE}
knitr::include_graphics("../../images/kmeans_5.jpg")
```


## Agenda <i class="fas fa-calendar-alt" target="_blank"></i>

### November 19, 2025

1. distances
2. hierarchical clustering

### November 24, 2025

1. k-means clustering
2. k-medoids clustering

### December 1, 2025

1. latent Dirichlet allocation (LDA)


## Readings <i class="fas fa-book-open"></i> 

* Class notes: <a href = "https://st47s.com/Math154/Notes/09-clustering.html" target= "_blank">Unsupervised Methods</a>

* Gareth, Witten, Hastie, and Tibshirani (2021), <a href = "https://www.statlearning.com/" target = "_blank">Unsupervised Learning (Chapter 12) </a> Introduction to Statistical Learning.


## Reflection questions <i class="fas fa-lightbulb"></i>

* Why does the plot of within-cluster sum of squares vs. $k$ make an elbow-shape (hint: think about $k$ as it ranges from 1 to $n)?$

* How are the centers of the clusters in $k$-means calculated?  What about in $k$-medoids?

* Will a different initialization of the cluster centers always produce the same cluster output?

* How do distance metrics change a hierarchical clustering?  

* How can you choose $k$ with hierarchical clustering?

* What is the difference between single, average, and complete linkage in hierarchical clustering?

* What is the difference between agglomerative and divisive hierarchical clustering?

## Ethics considerations <i class="fas fa-balance-scale"></i> 

* What type of feature engineering is required for $k$-means / hierarchical clustering?

* How do you (can you?) know if your clustering has uncovered any real patterns in the data?

## Slides <i class="fas fa-desktop"></i> 

* <a href = "https://m154-comp-stats.netlify.app/slides/2025-11-17-cluster" target = "_blank">In class slides - clustering</a> for 11/19/25 + 12/24/25.

* <a href = "https://m154-comp-stats.netlify.app/slides/2025-12-01-lda" target = "_blank">In class slides - latent Dirichlet allocation</a> for 12/1/25.


* <a href = "https://m154-comp-stats.netlify.app/handout/ws19_m154_f25_hierarch.pdf" target = "_blank">WS 19 - hierarchical clustering</a>

* <a href = "https://m154-comp-stats.netlify.app/handout/ws20_m154_f25_kmeans.pdf" target = "_blank">WS 20 - k-means clustering</a>

## Additional Resources <i class="fas fa-laptop"></i> 

* <a href = "https://www.naftaliharris.com/blog/visualizing-k-means-clustering/" target = "_blank">Fantastic k-means applet</a> by Naftali Harris.

* <a href = "http://varianceexplained.org/r/love-actually-network/" target = "_blank">Analyzing networks of characters in 'Love Actually'</a>



